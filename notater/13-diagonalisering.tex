\input{kapittel}

\kapittel{13}{Diagonalisering}
\label{ch:diagonalisering}
Dette kapitlet avslutter det skal lære om egenverdier og egenvektorer.


\section*{Hvorfor heter det diagonalisering?}
La $A$ være en  $n \times n$-matrise med $m$ lineært uavhengige egenvektorer
$\V{v}_1$, $\V{v}_2\dots \V{v}_m$ 
og tilhørende egenverdier $\lambda_1$, $\lambda_2$,$\cdots$, $\lambda_m$, . 
For hver egenvektor gjelder
\[
A\V{v}_k=\lambda_k \V{v}_k.
\]
Disse $m$ ligningene kan like gjerne organiseres i en matriseligning
\[
AV=VD, 
\]
der 
\[
D=
\begin{bmatrix}
\lambda_1      & 0      & 0      & \cdots & 0 \\
0      & \lambda_2      & 0      & \cdots & 0 \\
0      & 0      & \lambda_3      & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0      & 0      & 0      & \cdots & \lambda_m
\end{bmatrix}
\]
og 
\[
V=
\begin{bmatrix}
\V{v}_1 & \V{v}_2 & \cdots & \V{v}_m
\end{bmatrix}.
\]

Siden $\V{v}_1$, $\V{v}_2\dots \V{v}_n$ er lineært uavhengige, er matrisen $V$  invertibel dersom $m=n$.
Vi ganger med $V^{-1}$ fra høyre og får
\[
V^{-1}AV=D.
\]
Denne operasjonen kalles å \emph{diagonalisere} $A$, og dette er grunnen til at en $n\times n$-matrise med $n$ lineært uavhengige egenvektorer kalles diagonaliserbar. 
Man kan også se det motsatt. Dersom 
\[
V^{-1}AV=D
\]
for en inverterbar $n \times n$-matrise $V$, kan vi gange fra venstre med $V$, og få
\[
AV=VD
\]
Vi ser av denne likningen at kolonnene til $V$ utgjør $n$ lineært uavhengige vektorer for $A$.

\begin{thm}
Vi kan skrive 
\[
V^{-1}AV=D
\]
hvis og bare hvis $A$ har $n$ lineært uavhengige egenvektorer.
\end{thm}

\begin{ex}	
Vi diagonaliserer matrisen 
\[
A=
\begin{bmatrix}
2     & 1 \\
1      & 2
\end{bmatrix}.
\]
Egenvektorene er 
\[
\begin{bmatrix}
1 \\ 1
\end{bmatrix}
\quad \text{og} \quad
\begin{bmatrix}
1 \\ -1
\end{bmatrix},
\]
med respektive egenverdier 3 og 1. Vi definerer
\[
V=
\begin{bmatrix}
1 & 1 \\ 1 &-1
\end{bmatrix},
\]
og beregner 
\[
V^{-1}=
\frac{1}{2}
\begin{bmatrix}
1 & 1 \\ 1 &-1
\end{bmatrix}.
\]
Vi dobbeltsjekker ved å beregne produktet
\begin{align*}
V^{-1}AV&=
\frac{1}{2}
\begin{bmatrix}
1 & 1 \\ 1 &-1
\end{bmatrix}
\begin{bmatrix}
2     & 1 \\
1      & 2
\end{bmatrix}
\begin{bmatrix}
1 & 1 \\ 1 &-1
\end{bmatrix}
\\& =
\begin{bmatrix}
3     & 0 \\
0      & 1
\end{bmatrix}
=
D \qedhere
\end{align*}
\end{ex}


\begin{ex}
Matrisen 
\[
\begin{bmatrix}
1 & 1 & 0\\  0 &1 & 0 \\ 0 & 0 & 2
\end{bmatrix}
\]
har bare to lineært uavhengige egenvektorer, og er følgelig ikke diagonaliserbar.
\end{ex}

Man kan også snu likningen
\[
D=V^{-1}AV
\]
om, og få
\[
A=VDV^{-1},
\]
og dermed tenke på diagonalisering som en faktorisering av $A$.

\begin{ex}
Vi kan faktorisere matrisen 
\[
\begin{bmatrix}
2     & 1 \\
1      & 2
\end{bmatrix}
\]
som
\[
\begin{bmatrix}
2     & 1 \\
1      & 2
\end{bmatrix}
=
\begin{bmatrix}
1 & 1 \\ 1 &-1
\end{bmatrix}
\begin{bmatrix}
3     & 0 \\
0      & 1
\end{bmatrix}
\frac{1}{2}
\begin{bmatrix}
1 & 1 \\ 1 &-1
\end{bmatrix}.\qedhere
\]
\end{ex}
\begin{merkx}
I eksemplet over spiller ikke plasseringen av $V$ og $V^{-1}$ noen rolle. Mer om dette under.
\end{merkx}

\section*{Kvadratiske former}
La $\V v$ være en kolonnevektor i $\C^n$ og $A$ en $n\times n$-matrise. En kvadratisk form er et uttrykk på formen 
\[
\V v^* A \V v
\]
Et slikt uttrykk er spesielt interessant dersom $\V v$ er en egenvektor med lengde 1 og egenverdi $\lambda$:
\[
\V v^* A \V v=\V v^* \lambda \V v=\lambda \V v^* \V v=\lambda
\]



\section*{Symmetriske matriser}
En matrise sies å være 
\defterm{symmetrisk} dersom $A=A^*$.

\begin{ex}
Matrisen
\[
\begin{bmatrix}
1 & 1+i & -i\\  1-i &0 & 2-i \\ i & 2+i  & 2
\end{bmatrix}
\]
er symmetrisk.
\end{ex}

\begin{merkx}
En symmetrisk matrise har reelle diagonalelementer.
\end{merkx}

\begin{defnx}
En $n \times n$-matrise er \defterm{ortogonalt diagonaliserbar} dersom den har $n$ ortogonale egenvektorer.
\end{defnx}

Dersom vi lar $V$ være en $n \times n$-matrise bestånde av $A$ sine $n$ ortonormale egenvektorer, 
ser vi at 
\[
V^* A V =V^* VD =D
\]
siden $V^* V=I$. Motsatt ser vi at dersom
\[
V^* A V =D,
\]
for en ortonormal matrise $V$, utgjør $V$ sine kolonner  $n$ ortonormale egenvektorer for $D$.

\begin{thm}
Vi kan skrive 
\[
V^*AV=D
\]
hvis og bare hvis $A$ har $n$ ortogonale egenvektorer. 
\end{thm}


\begin{thm}
Dersom  $A=A^*$, er 
$
\V x^* A \V x
$
reell.
\end{thm}
\begin{proof}
Vi ser fryktelig lett at
\[
\V x^* A \V x
\]
er en sum av reelle tall og komplekskonjugater.
\end{proof}


\begin{thm}
En  symmetrisk matrise har reelle egenverdier.
\end{thm}
\begin{proof}
La $\V v$ være en normalisert egenvektor med egenverdi $\lambda$. Vi vet at 
\[
\V v^* A \V v=\lambda,
\]
og venstresiden er reell, så da må også $\lambda$ være det.
\end{proof}



\begin{thm}
Egenvektorene til to distinkte egenverdier er ortogonale for symmetriske matriser.
\end{thm}

\begin{proof}
La $\V v_1$ og $\V v_2$ være to egenvektorer med egenverdier $\lambda_1$ og $\lambda_2$. 
Vi beregner (husk at $\lambda_1$ og $\lambda_2$ er reelle)
\begin{align*}
\lambda_1 \V v_1^*\V v_2&=(\lambda_1 \V v_1)^*\V v_2=(A \V v_1)^*\V v_2\\&= \V v_1^*A^*\V v_2=\V v_1^*(A\V v_2)=\V v_1^*(\lambda_2\V v_2)=\lambda_2 \V v_1^*\V v_2
\end{align*}
Vi vet altså at
\[
0= \lambda_1 \V v_1^*\V v_2-\lambda_2 \V v_1^*\V v_2=(\lambda_1-\lambda_2) \V v_1^*\V v_2,
\]
og hvis vi bruker at $\lambda_1$ og $\lambda_2$ er forskjellige, må vi ha $\V v_1^*\V v_2=0$.
\end{proof}

\noindent Det siste teoremet vi tar med, skal vi la stå ubevist. 
\begin{thm}
En  symmetrisk matrise er ortogonalt diagonaliserbar.
\end{thm}

\begin{ex}
Matrisen 
\[
\begin{bmatrix}
1 & 2 & 2\\  2 &6 & 2 \\ 2 & 2 & 6
\end{bmatrix}
\]
har egenverdier 4, 9 og 0. Egenvektorer er 
\[
\begin{bmatrix}
-4 \\ 1 \\ 1
\end{bmatrix}
\]
Nå våknet ungen, så jeg rakk ikke regne ut de to andre. Vi får ta det i forelesning.
%har egenverdier $0$ og $6\pm \sqrt{55}$, og følgelig tre lineært uavhengige egenvektorer. Den er derfor diagonaliserbar.
\end{ex}

\begin{merkx}
Det forrige eksemplet viser at en matrise kan være diagonaliserbar uten å være inverterbar.
\end{merkx}


%\begin{proof}
%Vi ser lett at en symmertrisk $n\times n$-matrise like mange ortogonale egenvektorer som distinkte egenverdier, og at vi kan finne en ortogonal basis for repeterte egenverdier.
%\end{proof}





\kapittelslutt
