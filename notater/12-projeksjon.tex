\input{kapittel}

\kapittel{12}{Projeksjon}
\label{ch:projeksjon}

En projeksjon er en lineærtransformasjon $P$ som tilfredsstiller
\[
 P\V x=P^2\V x.
 \]
 for alle $\V x$. Denne ligningen sier at intet nytt skjer om du benytter lineærtransformasjonen for andre gang,
 og man kan tenke at $ P\V x$ er skyggen $\V x$ kaster dersom man lyser på $\V x$ med en lommelykt.
Vi skal begrense oss til å studere ortogonale projeksjoner. 
Dette betyr at lommelykten står slik at $\V x$ og $P\V x$ danner en rettvinklet trekant.

 
 \section*{Ortogonal projeksjon i $\R^2$}
 Du husker skalarproduktet fra gymnaset.
  Du har lært to måter å beregne skalarproduktet, nemlig
\[
 \V{v}\cdot  \V{w} = \|\V v\| \|\V w\| \cos \theta,
 \] 
der $\theta$ er vinkelen mellom $\V{v}$ og $\V{w}$, og 
\[
 \V{v}\cdot  \V{w} = v_1 w_1+v_2w_2.
 \] 
Vi bruker skalarproduktet til å projisere vektorer ortogonalt på hverandre. Det sentrale spørsmålet er: hvordan kan vi skrive vektoren $\V w_{\V{v}}$ i figuren under? 
 \begin{center}
\begin{tikzpicture}[scale=.42]
\draw[-latex,thick] (0,0) -- (7,1);
\draw[-latex,thick] (0,0) -- (3,6);
\draw[-latex, thick] (0,0) -- (27/50*7,27/50*1);
\draw[-latex,thick] (27/50*7,27/50*1) -- (3,6);
\draw[-]  (27/50*7-1/7,27/50*1+1-1/77) -- (27/50*7+1-1/7,27/50*1+8/7-1/77);
\draw[-]  (27/50*7+1,27/50*1+1/7) -- (27/50*7+1-1/7,27/50*1+8/7-1/77);
\node[anchor=east] at (9,1.2) {\footnotesize $\V{v}$};
\node[anchor=south] at (3.5,7) {\footnotesize $\V{w}$};
\node[anchor=east] at (3,-.5) {\footnotesize $\V w_{\V{v}}$};
\node[anchor=west] at (4.0,3) {\footnotesize $\V w-\V w_{\V{v}}$};
%\foreach \x in {-4,-3,-2,-1,1,2,3,4,5,6}
%\draw (\x,5pt) -- (\x,-5pt);
%\foreach \y in {-4,-3,-2,-1,1,2,3,4,5}
%\draw (5pt,\y) -- (-5pt,\y);
%\filldraw (2,3) circle [radius=3pt] node[anchor=west] {$z=2+3i$};
%\filldraw (2,-3) circle [radius=3pt] node[anchor=west] {$\overline z=2-3i$};
%\filldraw (4,5) circle [radius=3pt] node[anchor=west] {$w=4+5i$};
%\filldraw (0,1) circle [radius=3pt] node[anchor=east] {$\V{e}_2$};
%\filldraw (-1,-2) circle [radius=3pt] node[anchor=east] {$\V{u}$};
%\filldraw (3,2) circle [radius=3pt] node[anchor=east] {$\V{v}$};
%\filldraw (1,4) circle [radius=3pt] node[anchor=south] {$A \V{e}_1$};
%\filldraw (3,-3) circle [radius=3pt] node[anchor=north] {$A \V{e}_2$};
%\filldraw (-7,2) circle [radius=3pt] node[anchor=east] {$A \V{u}$};
%\filldraw (9,6) circle [radius=3pt] node[anchor=north] {$A \V{v}$};
%\draw[->,shorten <=4pt,shorten >=4pt] (1,0) to[bend right=20] (1,4);
%\draw[->,shorten <=4pt,shorten >=4pt] (0,1) to[bend right=30] (3,-3);
%\draw[->,shorten <=4pt,shorten >=4pt] (-1,-2) to[bend right=20] (-7,2);
%\draw[->,shorten <=4pt,shorten >=4pt] (3,2) to[bend left=20] (9,6);
\end{tikzpicture}
\\
{\small \textit{Hva er projeksjon?}}
\end{center}
%
% \begin{center}
%\begin{tikzpicture}[scale=.42]
%\draw[-latex,thick] (0,0) -- (7,1);
%\draw[-latex,thick] (0,0) -- (3,6);
%\draw[-, thick] (0,0) -- (27/50*7,27/50*1);
%%\draw[-, thick, red] (0,0) -- (27/50*7,27/50*1);
%\draw[-,thick] (27/50*7,27/50*1) -- (3,6);
%%\draw[-,thick, blue] (27/50*7,27/50*1) -- (3,6);
%\draw[-]  (27/50*7-1/7,27/50*1+1-1/77) -- (27/50*7+1-1/7,27/50*1+8/7-1/77);
%\draw[-]  (27/50*7+1,27/50*1+1/7) -- (27/50*7+1-1/7,27/50*1+8/7-1/77);
%\node[anchor=east] at (9,1.2) {\footnotesize $\V{v}$};
%\node[anchor=south] at (3.5,7) {\footnotesize $\V{w}$};
%\node[anchor=east] at (3,-.5) {\footnotesize $w_{\V{v}}$};
%%\node[anchor=east,red] at (3,-.5) {\footnotesize $w_{\V{v}}$};
%%\node[anchor=east,blue] at (5.5,3) {\footnotesize $w_{\V{v}^{\perp}}$};
%\node[anchor=east] at (2.9,2) {\footnotesize $\theta$};
%\draw (2.5,.4) arc (8:55:3);
%%\foreach \x in {-4,-3,-2,-1,1,2,3,4,5,6}
%%\draw (\x,5pt) -- (\x,-5pt);
%%\foreach \y in {-4,-3,-2,-1,1,2,3,4,5}
%%\draw (5pt,\y) -- (-5pt,\y);
%%\filldraw (2,3) circle [radius=3pt] node[anchor=west] {$z=2+3i$};
%%\filldraw (2,-3) circle [radius=3pt] node[anchor=west] {$\overline z=2-3i$};
%%\filldraw (4,5) circle [radius=3pt] node[anchor=west] {$w=4+5i$};
%%\filldraw (0,1) circle [radius=3pt] node[anchor=east] {$\V{e}_2$};
%%\filldraw (-1,-2) circle [radius=3pt] node[anchor=east] {$\V{u}$};
%%\filldraw (3,2) circle [radius=3pt] node[anchor=east] {$\V{v}$};
%%\filldraw (1,4) circle [radius=3pt] node[anchor=south] {$A \V{e}_1$};
%%\filldraw (3,-3) circle [radius=3pt] node[anchor=north] {$A \V{e}_2$};
%%\filldraw (-7,2) circle [radius=3pt] node[anchor=east] {$A \V{u}$};
%%\filldraw (9,6) circle [radius=3pt] node[anchor=north] {$A \V{v}$};
%%\draw[->,shorten <=4pt,shorten >=4pt] (1,0) to[bend right=20] (1,4);
%%\draw[->,shorten <=4pt,shorten >=4pt] (0,1) to[bend right=30] (3,-3);
%%\draw[->,shorten <=4pt,shorten >=4pt] (-1,-2) to[bend right=20] (-7,2);
%%\draw[->,shorten <=4pt,shorten >=4pt] (3,2) to[bend left=20] (9,6);
%\end{tikzpicture}
%\\
%{\small \textit{Hva er projeksjon?}}
%\end{center}
Vi kan utlede en formel for lengden:
\[
\|\V w_{\V{v}}\|=\|\V w\| \cos \theta=\frac{\|\V v\|}{\|\V v\|} \|\V w\| \cos \theta =\frac{\V v \cdot \V w}{\|\V v\|},
\]
%og $\V w$s komponent ortogonalt på $\V v$, altså den blå lengden:
%\[
%w_{\V{v}^{\perp}}=\sqrt{\|\V w\|^2-\left(\frac{\V v \cdot \V w}{\|\V v\|}\right)^2}.
%\]
%Denne lengden kalles $\V v$ sin skalarprojeksjon ortogonalt på $\V w$. 
slik at
\[
\V w_{\V{v}}=\|\V w_{\V{v}}\|\frac{\V v}{\|\V v\|}=\frac{\V v \cdot \V w}{\|\V v\|^2}\V v=\frac{\V v \cdot \V w}{\V v \cdot \V v}\V v.
\]
Denne vektoren kalles gjerne $\V w$ sin komponent i retningen gitt av $\V v$, eller $\V w$ sin projeksjon på $\V v$. Komponenten til $\V w$ ortogonalt på $\V v$ er 
%og
%\[
%\V w - \V w_{\V{v}}.
%\]
%Disse to vektorene kalles henholdsvis projeksjonen av $\V w$ på $\V v$, og projeksjonen av $\V w$ ortogonalt på $\V v$.
%Tenker man på projeksjonene til $\V w$ på $\V v$ og $\V v^{\perp}$ som en lengder, 
%kalles det skalarprojeksjon, 
%og tenker man på det som vektorer, kalles det vektorprojeksjon. 

\begin{ex}
Vektoren
\[
\V w
=
\begin{bmatrix}
2 \\ 1
\end{bmatrix}
\]
sin komponent i retningen gitt av
\[
\V v
=
\begin{bmatrix}
1 \\ 2
\end{bmatrix}
\]
er:
\[
\V w_{\V{v}}=\frac{\V v \cdot \V w}{\V v\cdot \V v}\V v=
\frac{4}{5}
\begin{bmatrix}
1 \\ 2
\end{bmatrix} 
%
%\[
%\frac{\V v \cdot \V w}{\|\V v\|}=\frac{2 \cdot 1+1\cdot 2}{\sqrt{2^2+1^2}}=\frac{4}{\sqrt{5}}
\]
VI kan også beregne lengden $\V w - \V w_{\V{v}}$:
\[
\V w - \V w_{\V{v}}=
\begin{bmatrix}
2 \\ 1
\end{bmatrix} 
-
\frac{4}{5}
\begin{bmatrix}
1 \\ 2
\end{bmatrix}
=
\frac{3}{5}
\begin{bmatrix}
2 \\ -1
\end{bmatrix} \qedhere
\]
\end{ex}
 
\section*{Adjungering}

Før vi kan generalisere projeksjon til $\C^n$, må vi utvide transponeringsoperasjonen litt. 

\begin{defn}
La
\[
A =
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}
\]
være en kompleks $m \times n$-matrise.  Den \defterm{adjungerte} av~$A$ er
$n \times m$-matrisen
\[
A^* =
\begin{bmatrix}
\overline a_{11} & \overline a_{21} & \cdots & \overline a_{m1} \\
\overline a_{12} & \overline a_{22} & \cdots & \overline a_{m2} \\
\vdots & \vdots & \ddots & \vdots \\
\overline a_{1n} & \overline a_{2n} & \cdots & \overline a_{mn}
\end{bmatrix}
\]
der radene og kolonnene i~$A$ er byttet om, og alt er komplekskonjugert.
\end{defn}

\begin{merkx}
Å adjungere en reell matrise er det samme som å transponere den.
\end{merkx}


\begin{ex}
Hvis vi lar $A$ være matrisen
\[
A =
\begin{bmatrix}
5i & 0 & -2i \\
3 & i &  4
\end{bmatrix},
\]
så er den adjungerte av~$A^*$ gitt ved:
\[
A^* =
\begin{bmatrix}
 -5i & 3 \\
 0 & -i \\
2i & 4
\end{bmatrix}
\]
Hvis vi adjungerer denne matrisen igjen, så kommer vi tilbake til
utgangspunktet:
\[
(A^*)^* = A\qedhere
\]
\end{ex}

\noindent Vi tar med noen regneregler for adjungering.

\begin{thm}
For enhver matrise~$A$ har vi at å adjungere to ganger gir den
opprinnelige matrisen:
\[
(A^*)^* = A
\]
Hvis $A$ og~$B$ er matriser sik at produktet~$AB$ er definert, så er
den adjungerte av produktet lik produktet av de adjungerte, i
motsatt rekkefølge:
\[
(AB)^* = B^* A^*
\]
\end{thm}


\section*{Indre- og ytreprodukt}
La $\V{v}$ og $\V{w}$ være kolonnevektorer i $\C^n$. 
\defterm{Indreproduktet} mellom dem er definert som:
\[
\V{v}^* \V{w}= \overline v_1 w_1 + \overline v_2 w_2 + \cdots +\overline v_n w_n
 \] 
 \defterm{Ytreproduktet} er:
\begin{align*}
 \V{w} \V{v}^*&=
\begin{bmatrix}
w_1 \overline v_1       & w_1 \overline v_2             & \cdots & w_1 \overline v_n \\
w_2 \overline v_1     & w_2 \overline v_2              & \cdots & w_2 \overline v_n   \\
\vdots & \vdots & \vdots  & \vdots \\
w_n \overline v_1     & w_n \overline v_2              & \cdots & w_n \overline v_n   \\
\end{bmatrix}
\\[3pt]&=
\begin{bmatrix}
\V w  \overline v_1 & \V w \overline v_2  & \hdots & \V w \overline v_n 
\end{bmatrix}
%+
%\overline v_2
%\begin{bmatrix}
%w_1  \\
%w_2  \\
%\vdots  \\
%w_n  \\
%\end{bmatrix}
%+...+
%\overline v_n
%\begin{bmatrix}
%w_1  \\
%w_2  \\
%\vdots  \\
%w_n  \\
%\end{bmatrix}
 \end{align*}  
 Vi har definert indre- og ytreprodukt for kolonnevektorer. 
 Det er ikke noe problem å sette opp tilsvarende definisjoner for rekkevektorer, 
 men det skal vi ikke plage dere med. 
 Vi definerer \defterm{lengden} til en vektor $\V v$ som 
 \[
 \|\V v\|=\sqrt{\V{v}^*\V{v}},
 \]
 og vi sier at $\V{v}$ og $\V{w}$ er \defterm{ortogonale} dersom 
\[
 \V{v}^*\V{w}=\V{w}^*\V{v} = 0.
 \] 
 

 
 \begin{merkx}
 Dersom $\V{v}$ og $\V{w}$ er reelle, blir indreproduktet 
\[
\V{v}^* \V{w}=\V{v}^T\V{w}= v_1w_1 + v_2w_2 + \cdots +v_nw_n=\V{v}\cdot  \V{w}
 \] 
 slik du er vant til fra gymnaset. Resultatet av dette produktet er en skalar, og det er derfor man gjerne kaller det skalarproduktet.  
 \end{merkx}
 
 
\begin{merkx}
$\V{v}^*\V{v}$ består av de kvadrerte absoluttverdiene til komponentene til $\V v$. 
\end{merkx}
 
 \begin{merkx}
 Ytreproduktet $ \V{w} \V{v}^*$ er en ikke inverterbar matrise, siden alle kolonnene er parallelle.
 \end{merkx}
 
 

 \begin{ex}
I $\R^2$ er $\V{v}$ og $\V{w}$ ortogonale dersom vinkelen mellom dem er $\pi/2$.
\end{ex}


 \begin{ex}
 Vektorene 
 \[
 \begin{bmatrix}
 1 \\ 
 1 \\
 -1\\
 0
 \end{bmatrix}
 \quad
 \text{og}
 \quad
  \begin{bmatrix}
 0 \\ 
 1 \\
 1\\
 1
 \end{bmatrix}
\]
er ortogonale. 
 \end{ex}

 \begin{ex}
 Vektorene 
 \[
 \begin{bmatrix}
 1 \\ 
 i 
 \end{bmatrix}
 \quad
 \text{og}
 \quad
  \begin{bmatrix}
 i \\ 
1
 \end{bmatrix}
\]
er ortogonale. 
 \end{ex}


 \begin{ex}
 Vektoren $\V{v}=\frac{\V{w}}{\|\V{w}\|}$ har lengde 1 for alle $\V w \neq 0$.
 \end{ex}
 
 Vi tar med noen regneregler for indre- og ytreprodukt. Disse er lette å utlede, så vi dropper bevisene.
\begin{thm}
Indreproduktet tilfredsstiller følgende regneregler:
\begin{align*}
 \V v ^* \V w&=\overline{\V w ^* \V v} \\
( c\V v )^* \V w&=\overline{c}(\V v ^* \V w ) = \V v ^* (\overline c\V w) \\
%  \V v ^* c\V w&=\overline c \V w ^* \V v \\
 (\V v+\V u) ^* \V w&= \V v ^* \V w+ \V u ^* \V w \\
  \V v ^* (\V w+\V u)&= \V v ^* \V w+ \V v ^* \V u
\end{align*}
\end{thm}
 
 Det neste på posten er Pytagoras' teorem. 

\begin{thm}
Dersom vektorene $\V v$ og $\V w$ er ortogonale, er
\[
\|\V v-\V w\|^2=\|\V v\|^2+\|\V w\|^2.
\]
\end{thm}

\begin{proof}
Vi vet at 
\begin{align*}
\|\V v-\V w \|^2&=(\V v-\V w)^*(\V v-\V w)\\&=\V v^*\V v-\V v^*\V w-\V w^*\V v+\V w^*\V w\\ & =\|\V v\|^2 -\V v^*\V w-\V w^*\V v+\|\V w\|^2\\ &=\|\V v\|^2 +\|\V w\|^2
\end{align*}
siden $\V v^*\V w-\V w^*\V v=0$.
siden $\v^* \w = 0$ og $\w^* \v = 0$.
\end{proof}

 \section*{Projeksjon i $\C^n$}
%En projeksjon er en lineærtransformasjon $P$ som tilfredsstiller
%\[
% P\V x=P^2\V x.
% \]
% for alle $\V x$. Denne ligningen sier at intet nytt skjer om du benytter lineærtransformasjonen for andre gang, 
% og det er nettopp den egenskapen vi er ute etter når vi projiserer. 
% 
% \begin{ex}
%Både nullmatrisen og identitetsmatrisen er projeksjoner.
% \end{ex}
% 
%\noindent Dersom $P$ er en projeksjon, er også $I-P$ det:
%\[
% (I-P)^2=I^2-2P+P^2=I-P
% \]
%Dette kalles komplementærprojeksjonen til $P$.
% 
%\begin{ex}
%Nullmatrisen er identitetsmatrisens  komplementærprojeksjon.
%\end{ex}
% 
% \noindent I $\C$ er det kun to trivielle projeksjoner. En generell lineærtransformasjon på $\C$ kan skrives 
% \[
% Tx=ax.
% \]
%De eneste transformasjonene på denne formen som tifredsstiller $P=P^2$, er
% \[
% Px=x
%\]
%og 
% \[
% Px=0.
%\]
%Disse to er ikke så interessante, så vi skal i det etterfølgende anta at antall dimensjoner $n\geq 2$.
%
%La oss si at $P$ har en egenverdi $\lambda$, med egenvektor $\V x$. I så fall må
%\[
%\lambda \V x=P\V x=P^2\V x=P(P\V x)=P(\lambda\V x)=\lambda P\V x=\lambda^2 \V x.
%\]
%Hver komponent i denne ligningen gir at
%\[
%\lambda=\lambda^2
%\]
%eller
%\[
%\lambda^2-\lambda=\lambda(\lambda-1)=0.
%\]
%Egenverdiene til en projeksjonsmatrise kan altså kun være  0 eller 1. 
% 
%\begin{fishythm}
%Interessante projeksjonsmatriser er ikke inverterbare.
%\end{fishythm}
%
%\begin{merkx}
%Identitets- og nullmatrisen er projeksjonsmatriser, men de er ikke spesielt interessante.
%\end{merkx}
%
%\begin{fishythm}
%Det finnes alltid vektorer som ikke endres av en bestemt projeksjon.
%\end{fishythm}
%
%\begin{merkx}
%Hvis du projiserer en vektor på seg selv, skjer det ikke så mye interessant.
%\end{merkx}
%
%
 
En naturlig generalisering av  projeksjon på $\V v \in \C^n$ er lineærtransformasjonen
 \[
 P_{\V v}=\frac{\V v \V v^*}{\V v^* \V v}.
 \]
% og ortogonal projeksjon ortogonalt på $\V v$ som lineærtransformasjonen
% \[
% P_{\V v^{\perp}}=I-\frac{\V v \V v^*}{\V v^* \V v}.
% \]
%Det er ikke så vanskelig å se at dette er generaliseringer av vektorprojeksjonsformlene i $\R^2$. 
Assosiativiteten til matrisemultiplikasjon gir at vi kan skrive
 \[
 P_{\V v}\V w=\frac{\V v \V v^*}{\V v^* \V v} \V w=\V v \frac{ \V v^*\V w}{\V v^* \V v}=\frac{ \V v^*\V w}{\V v^* \V v} \V v 
 \]
%og 
% \[
% P_{\V v^{\perp}}\V w=\left(I-\frac{\V v \V v^*}{\V v^* \V v}\right)\V w=\V w - \frac{ \V v^*\V w}{\V v^* \V v} \V v ,
% \]
 så likheten med projeksjon i $\R^2$ er slående. %Det er ikke så vanskelig å vise at $P_{\V v}\V w$ og $P_{\V v^{\perp}}\V w$ er ortogonale. Dette skal vi gjøre i øvingsopplegget.

 \begin{center}
\begin{tikzpicture}[scale=.42]
\draw[-latex,thick] (0,0) -- (7,1);
\draw[-latex,thick] (0,0) -- (3,6);
\draw[-latex, thick] (0,0) -- (27/50*7,27/50*1);
\draw[-latex,thick] (27/50*7,27/50*1) -- (3,6);
\draw[-]  (27/50*7-1/7,27/50*1+1-1/77) -- (27/50*7+1-1/7,27/50*1+8/7-1/77);
\draw[-]  (27/50*7+1,27/50*1+1/7) -- (27/50*7+1-1/7,27/50*1+8/7-1/77);
\node[anchor=east] at (9,1.2) {\footnotesize $\V{v}$};
\node[anchor=south] at (3.5,7) {\footnotesize $\V{w}$};
\node[anchor=east] at (3.5,-.8) {\footnotesize $P_{\V{v}}(\V w)$};
\node[anchor=west] at (4.0,3) {\footnotesize $\V w-P_{\V{v}}(\V w)$};
%\foreach \x in {-4,-3,-2,-1,1,2,3,4,5,6}
%\draw (\x,5pt) -- (\x,-5pt);
%\foreach \y in {-4,-3,-2,-1,1,2,3,4,5}
%\draw (5pt,\y) -- (-5pt,\y);
%\filldraw (2,3) circle [radius=3pt] node[anchor=west] {$z=2+3i$};
%\filldraw (2,-3) circle [radius=3pt] node[anchor=west] {$\overline z=2-3i$};
%\filldraw (4,5) circle [radius=3pt] node[anchor=west] {$w=4+5i$};
%\filldraw (0,1) circle [radius=3pt] node[anchor=east] {$\V{e}_2$};
%\filldraw (-1,-2) circle [radius=3pt] node[anchor=east] {$\V{u}$};
%\filldraw (3,2) circle [radius=3pt] node[anchor=east] {$\V{v}$};
%\filldraw (1,4) circle [radius=3pt] node[anchor=south] {$A \V{e}_1$};
%\filldraw (3,-3) circle [radius=3pt] node[anchor=north] {$A \V{e}_2$};
%\filldraw (-7,2) circle [radius=3pt] node[anchor=east] {$A \V{u}$};
%\filldraw (9,6) circle [radius=3pt] node[anchor=north] {$A \V{v}$};
%\draw[->,shorten <=4pt,shorten >=4pt] (1,0) to[bend right=20] (1,4);
%\draw[->,shorten <=4pt,shorten >=4pt] (0,1) to[bend right=30] (3,-3);
%\draw[->,shorten <=4pt,shorten >=4pt] (-1,-2) to[bend right=20] (-7,2);
%\draw[->,shorten <=4pt,shorten >=4pt] (3,2) to[bend left=20] (9,6);
\end{tikzpicture}
\\
{\small \textit{Hva er projeksjon?}}
\end{center}

% \begin{center}
%\begin{tikzpicture}[scale=.42]
%\draw[-latex,thick] (0,0) -- (7,1);
%\draw[-latex,thick] (0,0) -- (3,6);
%\draw[-latex, thick, red] (0,0) -- (27/50*7,27/50*1);
%\draw[-latex,thick, blue] (27/50*7,27/50*1) -- (3,6);
%\draw[-]  (27/50*7-1/7,27/50*1+1-1/77) -- (27/50*7+1-1/7,27/50*1+8/7-1/77);
%\draw[-]  (27/50*7+1,27/50*1+1/7) -- (27/50*7+1-1/7,27/50*1+8/7-1/77);
%\node[anchor=east] at (9,1.2) {\footnotesize $\V{v}$};
%\node[anchor=south] at (3.5,7) {\footnotesize $\V{w}$};
%\node[anchor=east,red] at (3.5,-.8) {\footnotesize $P_{\V{v}}(\V w)$};
%\node[anchor=east,blue] at (7,3) {\footnotesize $P_{\V{v}^{\perp}}(\V w)$};
%%\foreach \x in {-4,-3,-2,-1,1,2,3,4,5,6}
%%\draw (\x,5pt) -- (\x,-5pt);
%%\foreach \y in {-4,-3,-2,-1,1,2,3,4,5}
%%\draw (5pt,\y) -- (-5pt,\y);
%%\filldraw (2,3) circle [radius=3pt] node[anchor=west] {$z=2+3i$};
%%\filldraw (2,-3) circle [radius=3pt] node[anchor=west] {$\overline z=2-3i$};
%%\filldraw (4,5) circle [radius=3pt] node[anchor=west] {$w=4+5i$};
%%\filldraw (0,1) circle [radius=3pt] node[anchor=east] {$\V{e}_2$};
%%\filldraw (-1,-2) circle [radius=3pt] node[anchor=east] {$\V{u}$};
%%\filldraw (3,2) circle [radius=3pt] node[anchor=east] {$\V{v}$};
%%\filldraw (1,4) circle [radius=3pt] node[anchor=south] {$A \V{e}_1$};
%%\filldraw (3,-3) circle [radius=3pt] node[anchor=north] {$A \V{e}_2$};
%%\filldraw (-7,2) circle [radius=3pt] node[anchor=east] {$A \V{u}$};
%%\filldraw (9,6) circle [radius=3pt] node[anchor=north] {$A \V{v}$};
%%\draw[->,shorten <=4pt,shorten >=4pt] (1,0) to[bend right=20] (1,4);
%%\draw[->,shorten <=4pt,shorten >=4pt] (0,1) to[bend right=30] (3,-3);
%%\draw[->,shorten <=4pt,shorten >=4pt] (-1,-2) to[bend right=20] (-7,2);
%%\draw[->,shorten <=4pt,shorten >=4pt] (3,2) to[bend left=20] (9,6);
%\end{tikzpicture}
%\\
%{\small \textit{Hva er projeksjon III?}}
%\end{center}

\begin{ex}
La oss projisere vektoren 
\[
\V w=
\begin{bmatrix}
 -5i  \\
 0  \\
2i 
\end{bmatrix}
\]
både på og normalt på
\[
\V v=
\begin{bmatrix}
 3 \\
 -i \\
 4
\end{bmatrix}.
\]
Vi beregner:
\[
\V v^* \V v=3\cdot 3+i\cdot (-i)+4\cdot 4=26
\]
og
\[
\V v^* \V w = 3 \cdot (-5i) + i \cdot 0 + 4 \cdot 2i = -7i
\]
slik at
\begin{align*}
P_{\V v}\V w =\frac{ \V v^*\V w}{\V v^* \V v} \V v =
\frac{-7i}{26}
\begin{bmatrix}
 3 \\
 -i \\
 4
\end{bmatrix}
\end{align*}
og 
\begin{align*}
 \V w-P_{\V{v}}(\V w)&=\V w-\frac{ \V v^*\V w}{\V v^* \V v} \V v \\&= 
\begin{bmatrix}
 -5i  \\
 0  \\
2i 
\end{bmatrix}
-
\frac{-7i}{26}
\begin{bmatrix}
 3 \\
 -i \\
 4
\end{bmatrix}
=
\frac{1}{26}
\begin{bmatrix}
 -109i \\
 7 \\
 80i
\end{bmatrix}\qedhere
\end{align*} 
\end{ex}


\section*{Mer om ortogonalitet}
\begin{defnx}
En \defterm{ortogonal mengde} er en mengde vekorer $\V u_1$, $\V u_2$, ...,$\V u_n$, slik at
\[
\V u_i^*  \V u_k = 0
\]
for alle vektorer $\V u_i$ og $\V u_k$ i mengden. Dersom i tillegg $\|\V u_j \|=1$ for alle vektorene, sier vi at mengden er \defterm{ortonormal}.
\end{defnx}


\begin{ex}
Standardbasisen $\V e_1$, $\V e_2$,...,$\V e_n$ for $\C^n$ er en ortogonal mengde.
\end{ex}

\begin{defnx}
Dersom en ortogonal mengde $\V u_1$, $\V u _2$, ...,$\V u_n$ spenner ut et rom $V$, sier vi at mengden er en \defterm{ortogonal basis} for $V$.
\end{defnx}
\begin{defnx}
Det er vanlig å sette opp $\V u_1$, $\V u _2$, ...,$\V u_n$ som kolonner i en matrise $U$. Vi sier da at $U$ er en \defterm{ortogonal matrise}.
\end{defnx}
%\begin{ex}
%Mengden bliff blaff bloff er en ortogonal basis for $\C^3$.
%\end{ex}
%
%\begin{ex}
%Mengden bliff blaff bloff er en basis for $\C^3$, men den er ikke ortogonal.
%\end{ex}
%

Hvis vi har en ortogonal basis for et rom, er det veldig lett å finne en vektors komponenter i rommet. La oss si at vi ønsker å finne vektoren $\V v$ sine komponenter i basisen $\V u_1$, $\V u _2$, ...,$\V u_n$. Komponentene til $\V v$ er gitt ved likningen 
\[
\V v = x_1 \V u_1 + x_2 \V u_2 + ...x_n \V u_n = U\V x
\]
Hvis vi ganger begge sider av denne likningen med $U$, får vi
\[
U^* \V v = U^*U\V x,
\]
og siden kolonnene til $U$ er ortogonale, blir den kvadratiske matrisen $U^*U$ diagonal:
\[
U^*U=
\begin{bmatrix}
\V u_1^*\V u_1 & 0 & 0 & \hdots & 0 \\
0 & \V u_2^*\V u_2 & 0 & \hdots & 0 \\
0 & 0 & \V u_3^*\V u_3  & \hdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \hdots & \V u_n^*\V u_n
\end{bmatrix}.
\]
Følgelig er løsningen av systemet $U^* \V v = U^*U\V x$ enkel å skrive opp. 
\begin{thm}
La $\V u_1$, $\V u _2$, ...,$\V u_n$ være en ortogonal basis for $V$, og la $\V v \in V$. 
I så fall kan $\V v$ skrives
\begin{align*}
\V v&= P_{\V u_1} \V v + P_{\V u_2} \V v + ... +P_{\V u_n} \V v\\ &= \frac{\V u_1^*\V v}{\V u_1^*\V u_1}\V u_1 + \frac{\V u_2^*\V v}{\V u_2^*\V u_2}\V u_2+...+\frac{\V u_n^*\V v_n}{\V u_n^*\V u_n}\V u_n
\end{align*}
\end{thm}


Vi kan også projisere en vektor ned i et rom der den ikke hører hjemme. 
Projeksjonen minimerer avstanden fra rommet til vektoren.
\begin{thm}
La $\V u_1$, $\V u _2$, ...,$\V u_n$ være en ortogonal basis for $V$, og la $\V v \notin V$. Punktet
\begin{align*}
\V v'&= P_{\V u_1} \V v + P_{\V u_2} \V v + ... +P_{\V u_n} \V v\\ &= \frac{\V u_1^*\V v}{\V u_1^*\V u_1}\V u_1 + \frac{\V u_2^*\V v}{\V u_2^*\V u_2}\V u_2+...+\frac{\V u_n^*\V v_n}{\V u_n^*\V u_n}\V u_n
\end{align*}
er det punktet i $V$ som har kortest avstand til $\V v$:
\[
\|\V v-\V v' \|=\min_{\V w \in V} \|\V v-\V w \|
\]
\end{thm}
\begin{proof}
Vi må først bevise at $\V v-\V v'$ står ortogonalt på $V$. 
Rommet $V$ er utspent av $\V u_1$, $\V u _2$, ...,$\V u_n$.
Vi sjekker at $\V v-\V v'$ står ortogonalt på hver $\V u_j$:
\begin{align*}
(\V v-\V v')^*\V u_j&=\V v^*\V u_j-(\V v')^*\V u_j \\ &=\V v^*\V u_j-\V v^*\V u_j =0
\end{align*}
Dersom $\V w \in V$, ligger også $\V w-\V v'$ i $V$, og da står $\V w-\V v'$ og $\V v-\V v'$ ortogonalt på hverande. Pytagoras' teorem gir
\begin{align*}
\|\V v-\V w \|^2&=\|\V v-\V v'-(\V w-\V v') \|^2\\&= \|\V v-\V v' \|^2 +\|\V w-\V v' \|^2  \geq  \|\V v-\V v' \|^2,
\end{align*}
for alle $\V w \in V$, slik at 
\begin{align*}
\|\V v-\V w \| \geq  \|\V v-\V v' \|^,
\end{align*}
og
\[
\|\V v-\V v' \|=\min_{\V w \in V} \|\V v-\V w \|.\qedhere
\]
\end{proof}

%\begin{ex}
%Rekkerommet og nullrommet står ortogonalt på hverandre.
%\end{ex}

\section*{Gram-Schmidts metode}
La $\V v_1$, $\V v_2$, ...,$\V v_n$ være en lineært uavhengig vektormengde. 
Vi skal lage oss en ortogonal basis $\V u_1$, $\V u_2$, ...,$\V u_n$ for rommet utspent av vektorene i mengden. 
Vi begynner med å definere
\[
\V u_1=\V v_1
\]
Vektoren $\V v_2$ er ikke nødvendigvis ortogonal på $\V u_1$, men 
\[
\V u_2=\V v_2-P_{\V u_1} \V v_2=\V v_2-\frac{ \V u_1^*\V v_2}{\V u_1^* \V u_1}\V u_1
\]
er. 
Vektoren 
\begin{align*}
\V u_3&=\V v_3-P_{\V u_1} \V v_3-P_{\V u_2} \V v_3\\[2pt]&=\V v_3-\frac{ \V u_1^*\V v_3}{\V u_1^* \V u_1}\V u_1-\frac{ \V u_2^*\V v_3}{\V u_2^* \V u_2}\V u_2
\end{align*}
er ortogonal på både $\V u_1$ og $\V u_2$. 
De tre vektorene $\V u_1$, $\V u_2$ og $\V u_3$  
spenner ut det samme rommet som $\V v_1$, $\V v_2$ og $\V v_3$. 
Nå kan vi fortsette slik, og definere rekursivt
\begin{align*}
\V u_k&=\V v_k-\sum_{j=1}^{k-1}P_{\V u_j} \V v_k\\[2pt]&=\V v_k-\sum_{j=1}^{k-1}\frac{ \V u_j^*\V v_k}{\V u_j^* \V u_j}\V u_j.
\end{align*}
\begin{thm}
Mengden $\V u_1$, $\V u_2$, ...,$\V u_n$ er en ortogonal basis for rommet utspent av $\V v_1$, $\V v_2$, ...,$\V v_n$.
\end{thm}
\begin{proof}
Vi bruker induksjon.
Det er lett å se at  $\V u_1$ og $\V u_2$ er ortogonale: 
\begin{align*}
\V u_1^*\V u_2&=\V u_1^*(\V v_2-\frac{ \V u_1^*\V v_2}{\V u_1^* \V u_1}\V u_1) \\&=\V u_1^*\V v_2-\frac{ \V u_1^*\V v_2}{\V u_1^* \V u_1}\V u_1^*\V u_1 \\ &= \V u_1^*\V v_2-\V u_1^*\V v_2=0
\end{align*}
Siden $\V u_1$ og $\V u_2$ er ikketrivielle lineærkombinasjoner av de lineært uavhegnige vektorene $\V v_1$ og $\V v_2$, 
er det åpenbart at $\V u_1$ og $\V u_2$ spenner ut det samme rommet som $\V v_1$ og $\V v_2$. 

La nå 
\[
V_{k}=\Sp\{\V v_1,\V v_2,\dots,\V v_k\}.
\]
Vi antar at $\V u_1,\V u_2,\dots,\V u_{k-1}$ er en ortogonal basis for $V_{k-1}$. 
Vi må vise at $\V u_k$ står ortogonalt på $V_{k-1}$, og at $\V u_1,\V u_2,\dots,\V u_{k}$ spenner ut $V_k$.
Vi sjekker indreproduktet av $\V u_j$ med $\V u_k$. Siden $\V u_j^*\V u_m=0$ når $j\neq m$, får vi
\begin{align*}
\V u_j^*\V u_k&=\V u_j^*(\V v_k-\sum_{m=1}^{k-1} \frac{ \V u_m^*\V v_k}{\V u_m^* \V u_m}\V u_m) \\&=\V u_j^*\V v_k-\sum_{m=1}^{k-1} \frac{ \V u_m^*\V v_k}{\V u_m^* \V u_m}\V u_j^*\V u_m \\ &= \V u_j^*\V v_k-\V u_j^*\V v_k=0.
\end{align*}
Vi ser altså at $\V u_k$ står ortogonalt på alle $\V u_j$, og siden $\V u_1,\V u_2,\dots,\V u_{k-1}$ er en ortogonal basis for $V_{k-1}$, står $\V u_k$ ortogonalt på $V_{k-1}$. 
Siden $\V v_k$ er lineært uavhengig av $\V u_1,\V u_2,\dots,\V u_{k-1}$, og $\V u_k$ er en lineærkombinasjon av $V_k$ og $\V u_1,\V u_2,\dots,\V u_{k-1}$, 
spenner $\V u_1,\V u_2,\dots,\V u_{k}$ ut $V_k$.
\end{proof}



\section*{Minste kvadraters metode}
Dette er en teknikk for å finne tilnærmede løsninger til systemer med flere likninger enn ukjente. 
La oss si at $A$ er en $m \times n$-matrise, $\V x$ og $\V b$ er kolonnevektorer i $\C^n$, og at vi ønsker å betrakte systemet
\[
A\V x=\V b
\]
for $m>n$. Dette systemet vil ikke ha noen løsning med mindre $\V b$ tilfeldigvis ligger i kolonnerommet til $A$, 
så vi ønsker istedet å finne den $\V x$ som minimerer avstanden fra $A\V x$ til $\V b$. 
Hvis vi krever at vektoren $A\V x-\V b$ står ortogonalt på kolonnerommet til $A$, oppnår vi dette. Altså må vi ha
\[
A^*(A\V x-\V b)=\V 0
\]
eller 
\[
A^*A\V x=A^*\V b.
\]
Dette er et $n \times n$-system som kalles normalligningene. Løsningen av systemet gir den $\V x$ som minimerer avstanden fra $A\V x$ til $\V b$.
\begin{ex}
Vi ønsker å bruke minste kvadrats metode på systemet med totalmatrise
\[
\begin{amatrix}{2}
0 & 1   & 1-i\\
i & i &  1+i\\
 0 & i   & i
\end{amatrix}
\]
Vi ganger matrisen på venstre side av ligningssystemet med sin adjungerte
\[
\begin{bmatrix}
0 & -i   & 0\\
1 & -i &  -i
\end{bmatrix}
\]
og får 
\[
\begin{bmatrix}
0 & -i   & 0\\
1 & -i &  -i
\end{bmatrix}
\begin{bmatrix}
0 & 1\\
i & i \\
 0 & i
\end{bmatrix}
=
\begin{bmatrix}
1 & 1\\
 1 & 3
\end{bmatrix}.
\]
Vi ganger høyresiden med den adjungerte av venstresiden, og får
\[
\begin{bmatrix}
0 & -i   & 0\\
1 & -i &  -i
\end{bmatrix}
\begin{bmatrix}
 1-i\\
 1+i \\
 i
\end{bmatrix}
=
\begin{bmatrix}
 1-i\\
 3-2i
\end{bmatrix}
\]
Løsningen av systemet med totalmatrise
\[
\begin{amatrix}{2}
1 & 1   & 1-i\\
1 & 3 &  3-2i
\end{amatrix}
\]
er 
\[
\begin{bmatrix}
-i/  \\
1-i/2  
\end{bmatrix}.
\]
Dette betyr at vektoren 
\[
\begin{bmatrix}
0 & 1   \\
i & i   \\
 0 & i   
\end{bmatrix}
\begin{bmatrix}
-i/  \\
1-i/2  
\end{bmatrix}
=
\begin{bmatrix}
1-i/2    \\
3/2+i   \\
 1/2+i
\end{bmatrix}
\]
er det punktet i kolonnerommet til matrisen 
\[
\begin{bmatrix}
0 & 1   \\
i & i   \\
 0 & i   
\end{bmatrix}
\]
som minimerer avstanden til punktet
\[
\begin{bmatrix}
1-i\\
 1+i\\
 i
\end{bmatrix}\qedhere
\]
\end{ex}

\section*{Litt om polynominterpolasjon}
Hvis du har $n+1$ punkter $(x_i,y_i)$ i $\R^2$, der $x_i$ er forskjellig for alle punkter, vil det generelt være mulig å finne et reelt polynom 
\[
p(x)=a_nx^n+a_{n-1}x^{n-1}+...+a_1x+a_0
\]
hvis graf går gjennom alle disse punktene, altså at
\[
p(x_i)=y_i
\]
for alle $1\leq i \leq n+1$. Dette kalles \defterm{interpolasjon}. Likningene over utgjør et $(n+1)\times (n+1)$-likningssystem for koeffisientene $a_i$ med totalmatrise
\[
\begin{amatrix}{5}
x_{1}^{n} & x_{1}^{n-1} & \hdots  & x_{1} & 1 & y_1\\
x_{2}^{n} & x_{2}^{n-1} & \hdots  & x_{2} & 1 & y_2\\
\vdots  & \vdots & \ddots  & \vdots & \vdots & \vdots\\
x_{n+1}^{n} & x_{n+1}^{n-1} & \hdots  & x_{n+1} & 1 & y_{n+1}\\
\end{amatrix}
\]
Det kan vises at dette ligningssystemet alltid har entydig løsning så lenge $x_j\neq x_k$ for $j \neq k$, 
men det skal vi ikke gjøre. Det følger at du alltid kan interpolere $n+1$ punkter med et polynom av orden $n$ på en entydig måte. 


\begin{ex}
Vi prøver å finne et annengradspolyom som går gjennom punktene
\[
\begin{bmatrix}
0 \\ 1
\end{bmatrix}, \;
\begin{bmatrix}
1 \\ 0
\end{bmatrix} \;
\quad \text{og} \quad
\begin{bmatrix}
2 \\ 1
\end{bmatrix}.
\]
Et annengradspolynom skrives $p(x)=ax^2+bx+c$, så likningssystemet blir 
\[
\systeme{
                   c = 1,
    a + b + c  = 0,
    4a + 2b + c = 1
}
\]
Løsningen er $a=1$ , $b=-2$ og $c=1$, slik at polynomet blir $p(x)=x^2-2x+1=(x-1)^2$. Det er lett å sjekke at polynomet tar de rette verdiene i $x=0$, $x=1$ og $x=2$.
\end{ex}


\noindent Dersom man prøver å gjøre den samme prosessen med et polynom som har orden $m < n$ , vil man få det overbestemte $(n+1)\times (m+1)$systemet 
\[
\begin{amatrix}{5}
x_{1}^{m} & x_{1}^{n-1} & \hdots  & x_{1} & 1 & y_1\\
x_{2}^{m} & x_{2}^{n-1} & \hdots  & x_{2} & 1 & y_2\\
\vdots  & \vdots & \ddots  & \vdots & \vdots & \vdots\\
x_{n+1}^{m} & x_{n+1}^{n-1} & \hdots  & x_{n+1} & 1 & y_{n+1}\\
\end{amatrix}
\]
Bruker man så minste kvadrats metode på dette systemet, 
får man et polynom som passer ganske bra til punktene uten at grafen går gjennom hvert enkelt punkt - 
dette kalles \defterm{regresjon}.

\begin{ex}
Vi prøver å finne et annengradspolyom som går gjennom punktene
\[
\begin{bmatrix}
0 \\ 1
\end{bmatrix}, \;
\begin{bmatrix}
1 \\ 0
\end{bmatrix}, \;
\begin{bmatrix}
2 \\ 1
\end{bmatrix} \;
\quad \text{og} \quad
\begin{bmatrix}
3 \\ 2
\end{bmatrix} .
\]
Likningssystemet blir nå
\[
\systeme{
                   c = 1,
    a + b + c  = 0,
    4a + 2b + c = 1,
    9a + 3b + c = 2
}
\]
Dette systemet har ingen løsning, men vi kan bruke minste kvadrats metode. Matrisen er:
\[
A=
\begin{bmatrix}
0 & 0 & 1 \\  1&1 &1 \\ 4& 2&1 \\ 9& 3& 1
\end{bmatrix}, 
\]
mens høyresiden $\V b$ er:
\[
\V b=
\begin{bmatrix}
1 \\ 0 \\ 1 \\ 2
\end{bmatrix}.
\]
Den adjungerte $A^*$ er:
\[
A^*=
\begin{bmatrix}
0 & 1 & 4 & 9\\  0&1 &2 & 3\\ 1& 1&1 &1 
\end{bmatrix}.
\]
Vi ganger $A^*$ med $A$ og $\V b$, og får 
\[
A^*A=
\begin{bmatrix}
0 & 1 & 4 & 9\\  0&1 &2 & 3\\ 1& 1&1 &1 
\end{bmatrix}
\begin{bmatrix}
0 & 0 & 1 \\  1&1 &1 \\ 4& 2&1 \\ 9& 3& 1
\end{bmatrix} 
=
\begin{bmatrix}
98 & 36 & 14 \\  36 &14 & 3\\ 14& 6&4
\end{bmatrix}
\]
og 
\[
A^*\V b=
\begin{bmatrix}
0 & 1 & 4 & 9\\  0&1 &2 & 3\\ 1& 1&1 &1 
\end{bmatrix}
\begin{bmatrix}
1 \\ 0 \\ 1 \\ 2
\end{bmatrix} 
=
\begin{bmatrix}
22 \\ 8 \\ 3 
\end{bmatrix}
\]
Vi må løse systemet $A^*A=A^*\V b$, altså systemet med totalmatrise
\[
\begin{amatrix}{3}
98 & 36 & 14 & 22\\  36 &14 & 3 & 8\\ 14& 6&4 & 3
\end{amatrix}.
\]
Løsningen er 
\[
\begin{bmatrix}
0.27710843 \\ -0.13855422 \\ -0.01204819 
\end{bmatrix}
\]
slik at polynomet blir 
\[
p(x)=0.27710843x^2  -0.13855422x  -0.01204819. \qedhere
\]
\end{ex}
\kapittelslutt
