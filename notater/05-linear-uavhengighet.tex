\input{kapittel}

\kapittel{5}{Lineær uavhengighet}
\label{ch:linear-uavhengighet}


\section*{Definisjonen av lineær uavhengighet}

Vi starter med et eksempel:

\begin{ex}
\label{ex:lin-uavh-intro}
La $\V{u}$, $\V{v}$ og~$\V{w}$ være følgende tre vektorer i~$\R^3$:
\[
\V{u} = \vvv{4}{4}{9}
\qquad
\V{v} = \vvv{1}{1}{0}
\qquad
\V{w} = \vvv{1}{1}{1}
\]
Disse vektorene oppfyller følgende likhet (det er lett å sjekke):
\[
\V{u} = -5 \cdot \V{v} + 9 \cdot \V{w}
\]
En slik lineær likhet som knytter sammen vektorer tenker vi på som en
«avhengighet» mellom vektorene, og vi sier at vektorene $\V{u}$,
$\V{v}$ og~$\V{w}$ er lineært avhengige fordi det finnes en slik
sammenheng mellom dem.

Vi kan også skrive likheten vår på følgende måte ved å sette alle
vektorene på samme side av likhetstegnet:
\[
\V{u} + 5 \cdot \V{v} - 9 \cdot \V{w} = \V{0}
\]
Det vi har gjort nå er å skrive nullvektoren som en lineærkombinasjon
av $\V{u}$, $\V{v}$ og~$\V{w}$.  
\end{ex}

Hvis vi har en liste $\V{v}_1$, $\V{v}_2$, \ldots, $\V{v}_n$ med
vektorer, så er det klart at nullvektoren~$\V{0}$ er en
lineærkombinasjon av disse, fordi vi har likheten
\[
0 \cdot \V{v}_1 + 0 \cdot \V{v}_2 + \cdots + 0 \cdot \V{v}_n = \V{0}
\]
der vi har satt alle vektene til å være~$0$.

Men spørsmålet vi kan stille er: Kan vi også skrive $\V{0}$ som en
lineærkombinasjon av vektorene våre på en annen måte, der ikke alle
vektene er~$0$?  I eksempelet over kunne vi det, men i andre tilfeller
er det ikke mulig.  Dette er det vi vil bruke som definerende egenskap
for å si at noen gitte vektorer enten er lineært avhengige eller
lineært uavhengige.

\begin{defn}
La $\V{v}_1$, $\V{v}_2$, \ldots, $\V{v}_n$ være vektorer i $\R^m$.
Disse vektorene er \defterm{lineært uavhengige} dersom likningen
\[
\V{v}_1 \cdot x_1 + \V{v}_2 \cdot x_2 + \cdots + \V{v}_n \cdot x_n = \V{0}
\]
ikke har andre løsninger enn den trivielle løsningen
$x_1 = x_2 = \cdots = x_n = 0$.

I motsatt tilfelle kalles de \defterm{lineært avhengige}.
\end{defn}

\begin{ex}
Se på vektorene
\[
\V{v}_1 = \vvv{3}{-2}{0}
\qquad\text{og}\qquad
\V{v}_2 = \vvv{0}{0}{4}
\]
i $\R^3$.  Likningen
\[
\V{v}_1 x_1 + \V{v}_2 x_2 = \V{0}
\]
medfører at $3x_1 = 0$ og $4 x_2 = 0$, altså $x_1 = x_2 = 0$, så den
har bare den trivielle løsningen.  Dermed er $\V{v}_1$ og~$\V{v}_2$
lineært uavhengige.
\end{ex}

\begin{ex}
Vektorene $\V{u}$, $\V{v}$ og~$\V{w}$ i
eksempel~\ref{ex:lin-uavh-intro} er lineært avhengige, siden likningen
\[
\V{u} x_1 + \V{v} x_2 + \V{w} x_3 = \V{0}
\]
har ikketrivielle løsninger (for eksempel løsningen $x_1 = 1$,
$x_2 = 5$ og~$x_3 = -9$).
\end{ex}



\section*{Lineær uavhengighet for to vektorer}

Hvis vi ser på bare to vektorer, er det ikke vanskelig å sjekke om de
er lineært uavhengige eller ikke.

\begin{ex}
\label{ex:to-vektorer-lin-avh}
Vi undersøker om følgende vektorer er lineært uavhengige:
\[
\V{v}_1 = \vvv{8}{2}{-12}
\qquad\text{og}\qquad
\V{v}_2 = \vvv{4}{1}{-6}
\]
Siden vi har $\V{v}_1 = 2 \cdot \V{v}_2$, får vi at nullvektoren kan
skrives som en ikketriviell lineærkombinasjon av $\V{v}_1$ og~$\V{2}$
på denne måten:
\[
1 \cdot \V{v}_1 - 2 \cdot \V{v}_2 = \V{0}
\]
Vektorene $\V{v}_1$ og $\V{v}_2$ er derfor lineært avhengige.
\end{ex}

I dette eksempelet hadde vi at den ene vektoren kunne skrives som et
tall ganger den andre, og ut fra det fant vi at vektorene var lineært
avhengige.  Vi viser at dette generelt er nok til å bestemme om to
vektorer er lineært uavhengige eller ikke.

\begin{thm}
\label{thm:lin-uavh-2}
To vektorer $\V{u}$ og $\V{v}$ er lineært uavhengige hvis og bare
hvis ingen av dem er lik en skalar ganger den andre.
\end{thm}
\begin{proof}
Påstanden i teoremet kan også formuleres slik: Vektorene er lineært
\emph{avhengige} hvis og bare hvis en av dem er lik en skalar ganger
den andre.  Vi viser dette.

Anta først at $\V{u}$ og~$\V{v}$ er lineært avhengige.  Da finnes
to tall $a$ og~$b$ slik at
\[
\V{u} \cdot a + \V{v} \cdot b = \V{0}
\]
og minst én av $a$ og~$b$ er ulik~$0$.  Hvis $a \ne 0$, får vi
\[
\V{u} = - \frac{b}{a} \V{v}.
\]
Hvis $b \ne 0$, får vi
\[
\V{v} = - \frac{a}{b} \V{u}.
\]
I begge tilfeller har vi at én av vektorene er en skalar ganger den
andre.

Nå viser vi den motsatt implikasjonen.  Anta derfor at én av vektorene
er en skalar ganger den andre.  Hvis $\V{u} = c \cdot \V{v}$ for en
skalar~$c$, så får vi:
\[
\V{u} \cdot 1 + \V{v} \cdot (-c) = \V{0}.
\]
Hvis $\V{v} = d \cdot \V{u}$ for en skalar~$d$, så får vi:
\[
\V{u} \cdot d + \V{v} \cdot (-1) = \V{0}.
\]
I begge tilfeller har vi en ikketriviell løsning av likningen
$\V{u} \cdot x_1 + \V{v} \cdot x_2 = \V{0}$,
og det betyr at vektorene er lineært avhengige.
\end{proof}

Teoremet sier altså at to vektorer er lineært uavhengige hvis og bare
hvis de ikke ligger på en rett linje gjennom origo.

% Hvis vi har to vektorer i $\R^2$, er det to muligheter: Enten er de
% lineært avhengige, eller så utspenner de hele~$\R^2$.

% TODO: eksempel med figurer


\section*{Hvordan sjekke lineær uavhengighet}

Det å sjekke om vektorer $\V{v}_1$, $\V{v}_2$, \ldots, $\V{v}_n$ er
lineært uavhengige, er (fra definisjonen) det samme som å sjekke om
likningen
\[
\V{v}_1 \cdot x_1 + \V{v}_2 \cdot x_2 + \cdots + \V{v}_n \cdot x_n = \V{0}
\]
har uendelig mange løsninger, eller bare én.  Denne likningen kan vi
selvfølgelig løse på vanlig måte, ved å gausseliminere totalmatrisen:
\[
\begin{amatrix}{4}
\V{v}_1 & \V{v}_2 & \cdots & \V{v}_n & \V{0}
\end{amatrix}
\]


\begin{ex}
% TODO: omformuler eksempelet, bruk som introduksjon til teoremet
Er disse vektorene lineært uavhengige?
\[
\V{u} = \vvvv{3}{9}{3}{3},\qquad
\V{v} = \vvvv{2}{7}{2}{4}\qquad\text{og}\qquad
\V{w} = \vvvv{8}{31}{12}{22}
\]
Vi setter opp totalmatrisen for likningssystemet
\[
\V{u} \cdot x + \V{v} \cdot y + \V{w} \cdot z = \V{0},
\]
og gausseliminerer den:
\[
\begin{amatrix}{3}
3 & 2 & 8  & 0 \\
9 & 7 & 31 & 0  \\
3 & 2 & 12 & 0  \\
3 & 4 & 22 & 0 
\end{amatrix}
\sim
\begin{amatrix}{3}
3 & 2 & 8 & 0  \\
0 & 1 & 7 & 0  \\
0 & 0 & 4 & 0  \\
0 & 0 & 0 & 0 
\end{amatrix}
\]
Vi kunne fortsatt videre til redusert trappeform, men allerede her er
det tydelig at vi får kun én løsning: $x = y = z = 0$.  Dette betyr at
vektorene $\V{u}$, $\V{v}$ og~$\V{w}$ er lineært uavhengige.
\end{ex}

Det essensielle vi trenger å få ut av gausseliminasjonen for å sjekke
lineær uavhengighet, er om det blir noen frie variabler eller ikke.
Merk også at vi ikke egentlig trenger å ta med høyresidevektoren i
gausseliminasjonen.  I og med at det er bare $0$-er der fra
begynnelsen av, kan det aldri bli noe annet enn~$0$ der, uansett
hvilke radoperasjoner vi utfører.

Vi skriver opp et teorem basert på det vi har observert nå.

\begin{thm}
\label{thm:linuavh}
La $A$ være en matrise.  Følgende påstander er ekvivalente:
\begin{enumerate}
\item Kolonnene i $A$ er lineært uavhengige.
\item Likningen $A \V{x} = \V{0}$ har bare den trivielle løsn\-ingen
$\V{x} = \V{0}$.
\item Vi får ingen frie variabler når vi løser $A \V{x} = \V{0}$.
\item Når vi gausseliminerer $A$, får vi et pivotelement i hver kolonne.
\end{enumerate}
\end{thm}
\begin{proof}
Påstand~2 er bare en omskrivning av definisjonen av lineær
uavhengighet til en matriselikning.  Påstand~3 forklarer hvordan vi
kan se at likningen $A \V{x} = \V{0}$ ikke har mer enn én løsning
(husk at vi vet at den alltid har $\V{x} = \V{0}$ som løsning).
Påstand~4 er en omformulering av påstand~3, der vi utnytter at vi vet
at den siste kolonnen i totalmatrisen uansett bare består av $0$-er,
så vi trenger ikke ta den med i gausseliminasjonen.
\end{proof}

Teorem~\ref{thm:linuavh} gir oss en grei metode for å sjekke lineær
uavhengighet.  Hvis vi har vektorer
\[
\V{v}_1,\ \V{v}_2,\ \ldots,\ \V{v}_n
\]
i $\R^m$, kan vi finne ut om de er lineært uavhengige på denne måten:
\begin{enumerate}
\item Lag en matrise
$A = \begin{bmatrix} \V{v}_1 & \V{v}_2 & \ldots &
\V{v}_n \end{bmatrix}$ med disse vektorene som kolonner.
\item Gausseliminer $A$ til trappeform.
\item Hvis hver kolonne inneholder et pivotelement, er vektorene lineært uavhengige.
Ellers er de lineært avhengige.
\end{enumerate}
Når du gjør dette, bør du imidlertid ikke bare følge denne oppskriften
slavisk (og du må for all del ikke pugge slike fremgangsmåter som
dette), men huske på at det du egentlig gjør er å sjekke om likningen
\[
\V{v}_1 \cdot x_1 + \V{v}_2 \cdot x_2 + \cdots + \V{v}_n \cdot x_n = \V{0}
\]
har entydig løsning eller ikke.


\begin{ex}
Er disse vektorene lineært uavhengige?
\[
\V{v}_1 = \vvvv{5}{10}{5}{0},\qquad
\V{v}_2 = \vvvv{3}{7}{5}{4}\qquad\text{og}\qquad
\V{v}_3 = \vvvv{2}{6}{6}{8}
\]
Vi gausseliminerer matrisen $\begin{bmatrix} \V{v}_1 & \V{v}_2 & \V{v}_3 \end{bmatrix}$:
\[
\begin{bmatrix}
5 & 3 & 2 \\
10 & 7 & 6 \\
5 & 5 & 6 \\
0 & 4 & 8
\end{bmatrix}
\sim
\begin{bmatrix}
5 & 3 & 2 \\
0 & 1 & 2 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}
\]
Siste kolonne har ikke noe pivotelement.  Dermed er $\V{v}_1$,
$\V{v}_2$ og $\V{v}_3$ lineært avhengige.
\end{ex}


\begin{ex}
\label{ex:for-mange-vektorer}
Er disse vektorene lineært uavhengige?
\[
\V{v}_1 = \vvv{8}{7}{4},\quad
\V{v}_2 = \vvv{14}{-2}{5},\quad
\V{v}_3 = \vvv{3}{1}{0},\quad
\V{v}_4 = \vvv{7}{5}{11}\quad
\]
For å sjekke dette kan vi gausseliminere denne matrisen:
\[
\begin{bmatrix}
8 & 14 & 3 & 7 \\
7 & -2 & 1 & 5 \\
4 & 5 & 0 & 11
\end{bmatrix}
\]
Men vi trenger ikke egentlig å utføre gausseliminasjonen.  Vi ser med
en gang at uansett hva som skjer, så kan vi ikke få mer enn tre
pivotelementer (ett i hver rad).  Dermed kan det ikke bli
pivotelementer i alle de fire kolonnene, så vektorene $\V{v}_1$,
$\V{v}_2$, $\V{v}_3$ og $\V{v}_4$ er lineært avhengige.
\end{ex}

Som vi så i dette siste eksempelet, trenger vi ikke alltid å
gausseliminere for å finne ut om vektorer er lineært uavhengige eller
ikke.  Noen ganger kan vi se det på enklere måter.

Vi lister opp noen forskjellige betingelser som kan være nyttige å se
etter for å oppdage at vektorer er lineært \emph{avhengige}.

\begin{thm}
\label{thm:linavhspan}
Gitt $n$ vektorer  $\V{v}_1$, $\V{v}_2$, \ldots, $\V{v}_n$ i $\R^m$.  Hvis
\begin{enumerate}
\item en av vektorene er en lineærkombinasjon av de andre, eller
\item en av vektorene er $\V0$, eller
\item $n > m$,
\end{enumerate}
så er vektorene lineært avhengige.
\end{thm}
\begin{proof}
Anta først at én vektor~$\V{v}_k$ er en lineærkombinasjon av de andre:
\[
\V{v}_k = \sum_{i \ne k} a_i \V{v}_i
\]
Da kan vi sette $a_k = -1$ og få:
\[
\sum_{i = 1}^n a_i \V{v}_i = \V{0}
\]
Her har vi skrevet nullvektoren som en ikketriviell lineærkombinasjon
av vektorene våre (vi vet ikke hva alle $a_i$-ene er, men vi vet i
hvert fall at én av dem, $a_k$, ikke er~$0$).  Det betyr at vektorene
er lineært avhengige.

Nå går vi videre til å se på den andre antagelsen i teoremet, så vi
antar at én av vektorene i listen er nullvektoren.  Hvis
$\V{v}_k = \V{0}$, så kan vi definere $n$ tall $a_1$, $a_2$, \ldots,
$a_n$ ved:
\[
a_i =
\begin{cases}
0 & \text{hvis $i \ne k$} \\
1 & \text{hvis $i = k$}
\end{cases}
\]
Da får vi at
\[
\sum_{i = 1}^n a_i \V{v}_i = \V{0},
\]
og vektorene er lineært avhengige.

Til slutt ser vi på den tredje antagelsen.  Akkurat som i
eksempel~\ref{ex:for-mange-vektorer} får vi her at når vi
gausseliminerer matrisen
\[
\begin{bmatrix} \V{v}_1 & \V{v}_2 & \ldots & \V{v}_n \end{bmatrix},
\]
så får vi maksimalt $m$ pivotelementer (ett i hver rad), men vi
trenger $n$ pivotelementer (ett i hver kolonne) for at vektorene skal
være lineært uavhengige.  Når $n > m$ går ikke det an, så da er
vektorene lineært avhengige.
\end{proof}

Den første betingelsen i teorem~\ref{thm:linavhspan} er ikke bare
tilstrekkelig for å få lineær avhengighet, den er ekvivalent med at
vektorene er lineært avhengige.  Vi viser dette også.

\begin{thm}
\label{thm:lin-uavh-lin-komb}
Vektorene $\V{v}_1$, $\V{v}_2$, \ldots, $\V{v}_n$ i $\R^m$ er lineært
uavhengige hvis og bare hvis ingen av dem kan skrives som en
lineærkombinasjon av de andre.
\end{thm}
\begin{proof}
Påstanden i teoremet er det samme som å si at vektorene er lineært
avhengige hvis og bare hvis en av dem kan skrives som en
lineærkombinasjon av de andre.

I teorem~\ref{thm:linavhspan} viste vi at dersom en av vektorene er en
lineærkombinasjon av de andre, så er de lineært avhengige.  Det
gjenstår å vise at dersom vektorene er lineært avhengige, så er en av
dem en lineærkombinasjon av de andre.

Anta at vektorene er lineært avhengige, altså at vi har
\[
a_1 \V{v}_1 + a_2 \V{v}_2 + \cdots + a_n \V{v}_n = \V{0},
\]
der minst én av $a_i$-ene er ulik~$0$.  Velg en~$k$ slik at $a_k \ne 0$.
Da har vi:
\[
a_k \V{v}_k = \sum_{i \ne k} (-a_i) \V{v}_i
\]
Siden $a_k \ne 0$ kan vi dele på $a_k$ og få:
\[
\V{v}_k = \sum_{i \ne k} \frac{-a_i}{a_k} \cdot \V{v}_i
\]
Dermed er vektoren $\V{v}_k$ en lineærkombinasjon av de andre
vektorene i listen.
\end{proof}


\section*{Like mange vektorer som dimensjonen}

Til slutt ser vi på hva vi kan si om lineær uavhengighet hvis vi ser
på $n$ vektorer i~$\R^n$.  Nå har vi altså like mange vektorer som
dimensjonen til rommet vektorene bor i.

Hvis vi har to vektorer i~$\R^2$, så kan vi skille mellom følgende tre
tilfeller:
\begin{enumerate}
\item Begge vektorene er nullvektoren.  Da utspenner de bare mengden
bestående av nullvektoren, og de er lineært avhengige.
\item Minst én av vektorene er ulik~$\V{0}$, og vektorene ligger på
samme linje.  Da utspenner de denne linjen, og de er lineært
avhengige.
\item Vektorene ligger ikke på samme linje, de peker altså i hver sin
retning.  Da utspenner de hele planet, og de er lineært uavhengige.
\end{enumerate}

Hvis vi har tre vektorer i~$\R^3$, så kan vi skille mellom fire
tilfeller:
\begin{enumerate}
\item Alle er nullvektoren.  Da utspenner de bare mengden bestående av
nullvektoren, og de er lineært avhengige.
\item Minst én av vektorene er ulik~$\V{0}$, og vektorene ligger på
samme linje.  Da utspenner de denne linjen, og de er lineært
avhengige.
\item Vektorene ligger ikke på samme linje, men det finnes et plan
i~$\R^3$ som inneholder alle tre.  Da utspenner de dette planet, og de
er lineært avhengige.
\item Vektorene ligger ikke i samme plan.  Da utspenner de
hele~$\R^3$, og de er lineært uavhengige.
\end{enumerate}

Generelt har vi at $n$ vektorer i~$\R^n$ enten er lineært avhengige og
utspenner en mengde som er mindre enn~$\R^n$, eller så er de lineært
uavhengige og utspenner hele~$\R^n$.

\begin{thm}
\label{thm:linuavhspan}
Hvis vi har $n$ vektorer $\V{v}_1$, $\V{v}_2$, \ldots, $\V{v}_n$ i
$\R^n$, så er de lineært uavhengige hvis og bare hvis de utspenner
hele $\R^n$, altså hvis og bare hvis
\[
\Sp \{ \V{v}_1, \V{v}_2, \ldots, \V{v}_n \} = \R^n.
\]
\end{thm}
\begin{proof}
La
\[
A = \begin{bmatrix} \V{v}_1 & \V{v}_2 & \ldots & \V{v}_n \end{bmatrix}
\]
være $n \times n$-matrisen med vektorene våre som kolonner.  Vi vet at
vektorene er lineært uavhengige hvis og bare hvis vi får
pivotelementer i alle kolonner når vi gausseliminerer~$A$.  Men siden
$A$ er kvadratisk, er dette det samme som at vi får pivotelementer i
alle rader.  Det er igjen ekvivalent med at
\[
A \V{x} = \V{b}
\]
har løsning for alle vektorer~$\V{b}$ i~$\R^n$, som er det samme som
at kolonnene i~$A$ utspenner~$\R^n$.
\end{proof}


\kapittelslutt
