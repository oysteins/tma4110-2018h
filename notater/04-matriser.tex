\input{kapittel}

\kapittel{4}{Matriser}
\label{ch:matriser}

Nå har vi fått erfaring med å bruke matriser i et par forskjellige
sammenhenger.  Vi har lært å løse et lineært likningssystem ved å
sette opp totalmatrisen til systemet og gausseliminere den (ved hjelp
av radoperasjoner på matrisen), og vi har sett at et lineært
likningssystem kan skrives på formen
\[
A \V{x} = \V{b}
\]
der vi har samlet alle koeffisientene i matrisen~$A$.  For å få denne
likningen til å gi mening, definerte vi hvordan vi kan gange en
$m \times n$-matrise med en vektor i~$\R^n$ og få ut en vektor
i~$\R^m$.

I tillegg til dette matrise--vektor-produktet finnes det en hel rekke
andre aritmetiske operasjoner vi kan utføre på matriser.  I dette
kapitlet tar vi en grundig gjennomgang av disse operasjonene.


\section*{Definisjoner og notasjon}

%% TODO: definer kolonnevektor (søylevektor) og radvektor?

%% TODO: notasjon (v_1, ..., v_n) for kolonnevektor?

En \defterm{$m \times n$-matrise} er en rektangulær tabell med tall
som har $m$~tall i høyden og $n$~tall i bredden:
\[
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}
\]
\defterm{Kolonnene} i matrisen er følgende kolonnevektorer:
\[
\vvvv{a_{11}}{a_{21}}{\vdots}{a_{m1}}\qquad
\vvvv{a_{12}}{a_{22}}{\vdots}{a_{m2}}\qquad
\cdots\qquad
\vvvv{a_{1n}}{a_{2n}}{\vdots}{a_{mn}}\qquad
\]
\defterm{Radene} i matrisen er følgende radvektorer:
\begin{gather*}
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n}
\end{bmatrix}
\\
\begin{bmatrix}
a_{21} & a_{22} & \cdots & a_{2n}
\end{bmatrix}
\\
\vdots
\\
\begin{bmatrix}
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}
\end{gather*}

\begin{ex}
Her er et eksempel på en $2 \times 3$-matrise:
\[
\begin{bmatrix}
5 & 0 & -2 \\
3 & 1 &  4
\end{bmatrix}
\]
Kolonnene i denne matrisen er:
\[
\vv{5}{3},\quad
\vv{0}{1}\quad\text{og}\quad
\vv{-2}{4}
\]
Radene er:
\[
\begin{bmatrix}
5 & 0 & -2
\end{bmatrix}
\quad\text{og}\quad
\begin{bmatrix}
3 & 1 &  4
\end{bmatrix}
\qedhere
\]
\end{ex}

Noen ganger har vi en liste med kolonnevektorer, si
\[
\V{v}_1,\ \V{v}_2,\ \ldots,\ \V{v}_n,
\]
og vil lage en matrise som har disse vektorene som kolonner.
Den matrisen kan vi skrive slik:
\[
\begin{bmatrix}
\V{v}_1 & \V{v}_2 & \cdots & \V{v}_n
\end{bmatrix}
\]
Hvis vektorene ligger i~$\R^m$, blir dette en $m \times n$-matrise.

\begin{ex}
La
\[
\V{v}_1 = \vvv{2}{0}{4}
\qquad\text{og}\qquad
\V{v}_2 = \vvv{1}{1}{2}
\]
være to vektorer i~$\R^3$.  Matrisen
$\begin{bmatrix} \V{v}_1 & \V{v}_2 \end{bmatrix}$ med disse vektorene
som kolonner blir da følgende $3 \times 2$-matrise:
\[
\begin{bmatrix} \V{v}_1 & \V{v}_2 \end{bmatrix}
=
\begin{bmatrix}
2 & 1 \\
1 & 0 \\
2 & 4
\end{bmatrix}
\qedhere
\]
\end{ex}

På samme måte kan vi, hvis vi har en liste med radvektorer
\[
\V{r}_1,\ \V{r}_2,\ \ldots,\ \V{r}_m,
\]
lage en matrise
\[
\begin{bmatrix}
\V{r}_1 \\
\V{r}_2 \\
\vdots \\
\V{r}_m
\end{bmatrix}
\]
med disse vektorene som rader.

\begin{ex}
La
\[
\V{r}_1 = \begin{bmatrix} 2 & 1 \end{bmatrix},\quad
\V{r}_2 = \begin{bmatrix} 1 & 0 \end{bmatrix}
\quad\text{og}\quad
\V{r}_3 = \begin{bmatrix} 2 & 4 \end{bmatrix}
\]
være tre radvektorer.  Da har vi:
\[
\begin{bmatrix}
\V{r}_1 \\
\V{r}_2 \\
\V{r}_3
\end{bmatrix}
=
\begin{bmatrix}
2 & 1 \\
1 & 0 \\
2 & 4
\end{bmatrix}
\qedhere
\]
\end{ex}

Noen ganger vil vi bruke tilsvarende notasjon for å bygge opp en
matrise av andre matriser, eller av en kombinasjon av matriser og
vektorer.

\begin{ex}
\label{ex:ABv}
La $A$ og~$B$ være matriser, og $\V{v}$ en vektor, slik som dette:
\[
A =
\begin{bmatrix}
3 & 1 \\
0 & 2 \\
7 & 1
\end{bmatrix}
\qquad
B =
\begin{bmatrix}
 5 & 9 & 2 \\
-1 & 5 & 4 \\
10 & 0 & 3
\end{bmatrix}
\qquad
\V{v} = \vvv{6}{8}{-5}
\]
Da kan vi skrive $\begin{bmatrix} A & B & \V{v} \end{bmatrix}$ for
matrisen som inneholder alle tallene fra disse tre satt ved siden av
hverandre:
\[
\begin{bmatrix} A & B & \V{v} \end{bmatrix}
=
\begin{bmatrix}
3 & 1 &  5 & 9 & 2 & 6 \\
0 & 2 & -1 & 5 & 4 & 8 \\
7 & 1 & 10 & 0 & 3 & -5
\end{bmatrix}
\qedhere
\]
\end{ex}

En $n \times n$-matrise, altså en matrise med like mange rader og
kolonner, kaller vi for en \defterm{kvadratisk} matrise.  For eksempel
er matrisen~$B$ i eksempel~\ref{ex:ABv} en kvadratisk matrise, mens
$A$ ikke er det.



\section*{Produkt av matrise og vektor}

La
\[
A =
\begin{bmatrix}
\V{a}_1 & \V{a}_2 & \cdots & \V{a}_n
\end{bmatrix}
\]
være en $m \times n$-matrise med vektorene $\V{a}_1$, $\V{a}_2$,
\ldots, $\V{a}_n$ som kolonner, og la
\[
\V{v} = \vn{v}{n}
\]
være en vektor i~$\R^n$.  Vi definerer produktet $A\V{v}$ av $A$
og~$\V{v}$ som lineærkombinasjonen av kolonnene i~$A$ med tallene
i~$\V{v}$ som vekter:
\[
A \V{v} = \V{a}_1 v_1 + \V{a}_2 v_2 + \cdots + \V{a}_n v_n
\]
Merk at produktet~$A \V{v}$ bare er definert når bredden av matrisen
$A$ er lik høyden av vektoren~$\V{v}$.

\begin{ex}
\label{ex:Av}
Vi regner ut produktet av en $2 \times 3$-matrise og en vektor
i~$\R^3$:
\begin{align*}
\begin{bmatrix}
5 & 0 & -2 \\
3 & 1 &  4
\end{bmatrix}
\vvv{2}{-1}{3}
&=
\vv{5}{3} \cdot 2 +
\vv{0}{1} \cdot (-1) +
\vv{-2}{4} \cdot 3
\\
&= \vv{5 \cdot 2 + 0 \cdot (-1) + (-2) \cdot 3}
      {3 \cdot 2 + 1 \cdot (-1) +   4  \cdot 3}
\\
&= \vv{4}{17}
\end{align*}
Merk at resultatet blir en vektor i~$\R^2$.
\end{ex}

Når vi skal regne ut et produkt $A \V{v}$ av en matrise og en vektor,
trenger vi ikke egentlig å skrive opp lineærkombinasjonen av kolonnene
i $A$ med vekter fra $\V{v}$ slik som vi gjorde i eksempelet over.
Den andre linjen av utregningen i eksempelet viser en mer direkte måte
å komme frem på: Tallet som skal være på første posisjon i
resultatvektoren får vi ved å gange tallene fra første rad i~$A$ med
tallene i~$\V{v}$, og legge sammen resultatene.  Tallet på andre
posisjon i resultatvektoren får vi på samme måte fra andre rad i~$A$.

Generelt har vi at dersom
\[
A =
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}
\quad\text{og}\quad
\V{v} = \vn{v}{n},
\]
så kan vi regne ut produktet $A \V{v}$ på følgende måte:
\[
A \V{v} =
\vvvv{a_{11} v_1 + a_{12} v_2 + \cdots + a_{1n} v_n}
     {a_{21} v_1 + a_{22} v_2 + \cdots + a_{2n} v_n}
     {\vdots}
     {a_{m1} v_1 + a_{m2} v_2 + \cdots + a_{mn} v_n}
\]
(Det er ikke vanskelig å se at dette følger fra definisjonen
av~$A \V{v}$.)



\begin{thm}
\label{thm:Av-lin-komb}
Hvis $A$ er en $m \times n$-matrise, $\V{v}$ og~$\V{w}$ er vektorer
i~$\R^n$ og $c$ er et tall, så har vi følgende likheter:
\[
A (\V{v} + \V{w}) = A \V{v} + A \V{w}
\qquad\text{og}\qquad
A (c \V{v}) = c (A \V{v})
\]
\end{thm}

\begin{ex}
La $A$ og~$\V{v}$ være følgende matrise og vektor:
\[
A =
\begin{bmatrix}
5 & 0 & -2 \\
3 & 1 &  4
\end{bmatrix}
\quad\text{og}\quad
\V{v} = \vvv{2}{-1}{3}
\]
I eksempel~\ref{ex:Av} regnet vi ut produktet $A \V{v}$.
Men hva om vi har lyst til å gange $A$ med vektoren $(8, -4, 12)$?
Siden denne vektoren er lik $4 \cdot \V{v}$ kan vi bruke
teorem~\ref{thm:Av-lin-komb} og få:
\[
A \vvv{8}{-4}{12}
= A \cdot (4 \cdot \V{v})
= 4 \cdot (A \cdot \V{v})
= 4 \cdot \vv{4}{17}
= \vv{16}{68}
\]
Altså: $A$ ganget med vektoren $4 \V{v}$ er det samme som $4$ ganger
vektoren $A \V{v}$.
% \[
% \begin{bmatrix}
% 5 & 0 & -2 \\
% 3 & 1 &  4
% \end{bmatrix}
% \vvv{2}{-1}{3}
% = \vv{4}{17}
% \]
% Hva om vi vil gange den samme matrisen med vektoren $(8, -4, 12)$?
% Siden denne vektoren er lik $4 \cdot (2,-1,3)$, kan vi bruke
% teorem~\ref{thm:Av-lin-komb} og få:
% \begin{align*}
% \begin{bmatrix}
% 5 & 0 & -2 \\
% 3 & 1 &  4
% \end{bmatrix}
% \vvv{8}{-4}{12}
% &= 
% \begin{bmatrix}
% 5 & 0 & -2 \\
% 3 & 1 &  4
% \end{bmatrix}
% \left( 4 \cdot \vvv{2}{-1}{3} \right)
% \\
% &=
% 4 \cdot \left(
% \begin{bmatrix}
% 5 & 0 & -2 \\
% 3 & 1 &  4
% \end{bmatrix}
% \vvv{2}{-1}{3} \right)
% \\
% &=
% 4 \cdot \vv{4}{17}
% = \vv{16}{68}
% \end{align*}
\end{ex}

Det er verdt å merke seg hva som skjer hvis vi ganger en matrise med
en vektor der nøyaktig ett av tallene er~$1$, og resten er~$0$.  La
oss teste dette med en eksempelmatrise:

\begin{ex}
Vi ganger $2 \times 3$-matrisen
\[
A =
\begin{bmatrix}
5 & 0 & -2 \\
3 & 1 &  4
\end{bmatrix}
\]
med de tre vektorene
\[
\V{e}_1 = \vvv{1}{0}{0},\quad
\V{e}_2 = \vvv{0}{1}{0}\quad\text{og}\quad
\V{e}_3 = \vvv{0}{0}{1}
\]
og får følgende:
\begin{align*}
A \V{e}_1 &= \vv{5}{3} \\
A \V{e}_2 &= \vv{0}{1} \\
A \V{e}_3 &= \vv{-2}{4}
\end{align*}
Resultatene ble altså de tre kolonnene i~$A$.
\end{ex}

Generelt har vi at hvis $A$ er en $m \times n$-matrise, og $\V{e}_1$,
$\V{e}_2$, \ldots, $\V{e}_n$ er vektorene i~$\R^n$ som er slik at
$\V{e}_i$ har et $1$-tall i sin $i$-te koordinat og bare $0$-er
ellers, så er
\[
A \V{e}_1,\quad
A \V{e}_2,\quad
\ldots,\quad
A \V{e}_n
\]
nøyaktig samme vektorer som kolonnene i~$A$.


\section*{Sum og skalering av matriser}

La $A$ og~$B$ være to $m \times n$-matriser:
\[
A =
\begin{bmatrix}
a_{11} & \cdots & a_{1n} \\
a_{21} & \cdots & a_{2n} \\
\vdots & \ddots & \vdots \\
a_{m1} & \cdots & a_{mn}
\end{bmatrix}
\qquad
B =
\begin{bmatrix}
b_{11} & \cdots & b_{1n} \\
b_{21} & \cdots & b_{2n} \\
\vdots & \ddots & \vdots \\
b_{m1} & \cdots & b_{mn}
\end{bmatrix}
\]
% \[
% A =
% \begin{bmatrix}
% a_{11} & a_{12} & \cdots & a_{1n} \\
% a_{21} & a_{22} & \cdots & a_{2n} \\
% \vdots & \vdots & \ddots & \vdots \\
% a_{m1} & a_{m2} & \cdots & a_{mn}
% \end{bmatrix}
% \qquad
% B =
% \begin{bmatrix}
% b_{11} & b_{12} & \cdots & b_{1n} \\
% b_{21} & b_{22} & \cdots & b_{2n} \\
% \vdots & \vdots & \ddots & \vdots \\
% b_{m1} & b_{m2} & \cdots & b_{mn}
% \end{bmatrix}
% \]
Vi definerer summen $A+B$ på den mest åpenbare måten -- vi legger
sammen tallene fra de to matrisene i hver posisjon, og får en ny
$m \times n$-matrise:
\[
A + B =
\begin{bmatrix}
a_{11} + b_{11} & a_{12} + b_{12} & \cdots & a_{1n} + b_{1n} \\
a_{21} + b_{21} & a_{22} + b_{22} & \cdots & a_{2n} + b_{2n} \\
\vdots          & \vdots          & \ddots & \vdots          \\
a_{m1} + b_{m1} & a_{m2} + b_{m2} & \cdots & a_{mn} + b_{mn} \\
\end{bmatrix}
\]

Produktet av et tall og en matrise defineres også på den åpenbare
måten -- vi ganger med tallet på hver plass i matrisen:
\[
c A =
\begin{bmatrix}
c \cdot a_{11} & c \cdot a_{12} & \cdots & c \cdot a_{1n} \\
c \cdot a_{21} & c \cdot a_{22} & \cdots & c \cdot a_{2n} \\
\vdots         & \vdots         & \ddots & \vdots         \\
c \cdot a_{m1} & c \cdot a_{m2} & \cdots & c \cdot a_{mn}
\end{bmatrix}
\]

\begin{thm}
Hvis $A$ og~$B$ er $m \times n$-matriser, $\V{v}$ er en vektor
i~$\R^n$ og $c$ er et tall, så har vi følgende likheter:
\[
(A + B) \V{v} = A \V{v} + B \V{v}
\qquad\text{og}\qquad
(c A) \V{v} = c (A \V{v})
\]
\end{thm}


\section*{Vektorer som matriser}

Hittil har vi snakket om vektorer og matriser som to forskjellige
slags ting, men vi kan også velge å se på vektorer som et
spesialtilfelle av matriser der enten høyden eller bredden er~$1$.

En kolonnevektor
\[
\vn{v}{m}
\]
kan vi tenke på som en $m \times 1$-matrise, og en radvektor
\[
\begin{bmatrix} w_1 & w_2 & \cdots & w_n \end{bmatrix}
\]
kan vi tenke på som en $1 \times n$-matrise.

\begin{ex}
La $\V{v}$ og~$\V{w}$ være henholdsvis en kolonnevektor og en
radvektor:
\[
\V{v} = \vvv{5}{-2}{4}
\qquad
\V{w} = \begin{bmatrix} 2 & 3 & 1 \end{bmatrix}
\]
Vektoren~$\V{v}$ kan vi også se på som en $3 \times 1$-matrise, og
vektoren~$\V{w}$ kan vi se på som en $1 \times 3$-matrise.

Hvis vi velger å tenke på $\V{w}$ som en matrise og $\V{v}$ som en
vektor, så kan vi gange dem sammen med den vanlige regelen for produkt
av matrise og vektor:
\begin{align*}
\V{w} \cdot \V{v}
&= \begin{bmatrix} 2 & 3 & 1 \end{bmatrix} \cdot \vvv{5}{-2}{4} \\
&= \begin{bmatrix} 2 \cdot 5 + 3 \cdot (-2) + 1 \cdot 4 \end{bmatrix}
 = \begin{bmatrix} 8 \end{bmatrix}
\end{align*}
Resultatet blir vektoren~$\begin{bmatrix} 8 \end{bmatrix}$ i~$\R^1$.
En vektor i~$\R^1$ består av kun ett tall, og vi vil vanligvis si at
en slik vektor er det samme som det ene tallet.  Med andre ord kan vi
sløyfe klammene og ganske enkelt skrive:
\[
\V{w} \cdot \V{v} = 8.\qedhere
\]
\end{ex}

Generelt har vi at produktet av en radvektor og en kolonnevektor er
gitt ved følgende uttrykk:
\[
\begin{bmatrix} w_1 & w_2 & \cdots & w_n \end{bmatrix}
\cdot \vn{v}{n}
= w_1 v_1 + w_2 v_2 + \cdots + w_n v_n
\]
(Merk at det må være like mange tall i radvektoren som i
kolonnevektoren for at vi skal kunne gange dem sammen.)


\section*{Matrisemultiplikasjon}

Nå kommer vi til den mest spennende av de aritmetiske operasjonene vi
kan gjøre med matriser, nemlig \emph{matrisemultiplikasjon}.

Det er litt mer komplisert å beskrive hvordan vi multipliserer
matriser enn hvordan vi summerer dem.  Det er imidlertid en god grunn
til at det er slik.  Vi kunne valgt å definere multiplikasjon av
matriser på tilsvarende måte som sum, men det ville ikke blitt
spesielt nyttig.  Vi vil nemlig at matrisemultiplikasjon skal oppføre
seg pent sammen med multiplikasjon av matriser med vektorer.  Spesielt
vil vi at følgende likhet skal holde:
\[
(AB) \V{v} = A (B \V{v})
\]
Vi vil altså at vi fritt skal kunne flytte parentesene, akkurat slik
vi kan gjøre med et produkt av tre tall.

Hvordan kan vi definere produkt av matriser slik at dette fungerer?
La oss først se på et eksempel.

\begin{ex}
La $A$ og~$B$ være følgende to $2 \times 2$-matriser:
\[
A =
\begin{bmatrix}
3 & -5 \\
2 &  7
\end{bmatrix}
\qquad
B =
\begin{bmatrix}
4 & 3 \\
2 & 1
\end{bmatrix}
\]
Vi har lyst til å finne matrisen $AB$ som skal være slik at
$(AB) \V{v} = A(B(\V{v}))$ for alle vektorer $\V{v}$ i~$\R^2$.

Nå er det lurt å se på de to spesielle vektorene
\[
\vv{1}{0}
\quad\text{og}\quad
\vv{0}{1}.
\]
Vi vet fra tidligere at hvis vi ganger en $2 \times 2$-matrise med en
av disse vektorene, så får vi ut den første eller den andre kolonnen i
matrisen.

Vi regner ut:
\begin{align*}
B \vv{1}{0} &= \vv{4}{2} \\
A \left( B \vv{1}{0} \right)
&=
\begin{bmatrix}
3 & -5 \\
2 &  7
\end{bmatrix}
\vv{4}{2}
= \vv{2}{22}
\end{align*}
Det vi er ute etter er at $AB$ skal være slik at
\[
(AB)\V{v} = A(B\V{v})
\]
for alle vektorer~$\V{v}$.  Spesielt må vi da ha:
\[
(AB) \vv{1}{0}
= A \left( B \vv{1}{0} \right)
= \vv{2}{22}
\]
Men det å gange med vektoren
\[
\vv{1}{0}
\]
er det samme som å plukke ut første kolonne av matrisen, så vi har nå
funnet ut at første kolonne i matrisen~$AB$ må være:
\[
\vv{2}{22}
\]
På samme måte finner vi andre kolonne i~$AB$:
\begin{align*}
B \vv{0}{1} &= \vv{3}{1}
\\
A \left( B \vv{0}{1} \right)
&=
\begin{bmatrix}
3 & -5 \\
2 &  7
\end{bmatrix}
\vv{3}{1}
= \vv{4}{13}
\\
(AB) \vv{0}{1} &= A \left( B \vv{0}{1} \right) = \vv{4}{13}
\end{align*}
Dette betyr at andre kolonne i~$AB$ må være:
\[
\vv{4}{13}
\]
Dermed kommer vi frem til at produktet av $A$ og~$B$ er:
\[
AB =
\begin{bmatrix}
 2 &  4 \\
22 & 13
\end{bmatrix}
\qedhere
\]
\end{ex}

La oss nå generalisere det vi gjorde i eksempelet.  Foreløpig ser vi
på generelle $2 \times 2$-matriser, og så tar vi det helt generelle
tilfellet, med matriser av vilkårlig størrelse, etterpå.

La $A$ og~$B$ være to $2 \times 2$-matriser, og la
\[
\V{e}_1 = \vv{1}{0}
\quad\text{og}\quad
\V{e}_2 = \vv{0}{1}
\]
være de to spesielle vektorene vi brukte i eksempelet over.  La
$\V{b}_1$ og~$\V{b}_2$ være kolonnene i~$B$, slik at vi har:
\[
B = \begin{bmatrix} \V{b}_1 & \V{b}_2 \end{bmatrix}
\]
På samme måte som i eksempelet får vi nå:
\begin{align*}
(AB) \V{e}_1
&= A (B \V{e}_1)
 = A \V{b}_1 \\
(AB) \V{e}_2
&= A (B \V{e}_2)
 = A \V{b}_2
\end{align*}
Dette betyr at første kolonne i matrisen~$AB$ må være~$A \V{b}_1$, og
andre kolonne må være~$A \V{b}_2$.  Vi får altså:
\[
AB = \begin{bmatrix} A \V{b}_1 & A \V{b}_2 \end{bmatrix}
\]
Hvis vi lar $\V{a}_1$ og~$\V{a}_2$ være radene i~$A$, så gir dette oss
at:
\[
AB =
\begin{bmatrix}
\V{a}_1 \V{b}_1 & \V{a}_1 \V{b}_2 \\
\V{a}_2 \V{b}_1 & \V{a}_2 \V{b}_2
\end{bmatrix}
\]
For å virkelig gjøre dette detaljert, kan vi skrive opp nøyaktig
hvordan vi finner hvert tall i~$AB$ ut fra hvert enkelt av tallene i
$A$ og~$B$.  Hvis
\[
A =
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{bmatrix}
\quad\text{og}\quad
B =
\begin{bmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22}
\end{bmatrix},
\]
så får vi:
\[
AB =
\begin{bmatrix}
a_{11} b_{11} + a_{12} b_{21} & a_{11} b_{12} + a_{12} b_{22} \\
a_{21} b_{11} + a_{22} b_{21} & a_{21} b_{12} + a_{21} b_{22}
\end{bmatrix}
\]
Merk hvordan dette siste uttrykket er bygd opp.  Når vi skal finne
tallet som skal stå på en bestemt posisjon i~$AB$, går vi bortover den
tilsvarende \emph{raden} i~$A$ og samtidig nedover den tilsvarende
\emph{kolonnen} i~$B$.  Vi ganger sammen tallene vi finner i~$A$ med
de vi finner i~$B$, og legger sammen disse produktene.

Alt det vi gjorde nå fungerer helt tilsvarende når vi går til større
matriser enn $2 \times 2$.  Men for at det skal gå an å gange sammen
to matriser $A$ og~$B$, må de være «kompatible» i størrelse.  Vi
finner produktet~$AB$ ved å kombinere rader fra~$A$ med kolonner
fra~$B$, og for at dette skal gå an, må lengden av radene i~$A$ være
lik lengden av kolonnene i~$B$.  Det vil si at bredden til~$A$ må være
lik høyden til~$B$.

Basert på det vi har gjort nå lager vi en generell definisjon av
matrisemultiplikasjon.

\begin{defn}
La $A$ være en $m \times n$-matrise med rader
$\V{a}_1$, $\V{a}_2$, \ldots, $\V{a}_m$,
og la $B$ være en $n \times p$-matrise med kolonner
$\V{b}_1$, $\V{b}_2$, \ldots, $\V{b}_p$:
\[
A = \vn{\V{a}}{n}
\qquad\qquad
B = \begin{bmatrix} \V{b}_1 & \V{b}_2 & \cdots & \V{b}_p \end{bmatrix}
\]
Da er produktet av $A$ og~$B$ en $m \times p$-matrise definert ved:
\begin{align*}
AB &=
\begin{bmatrix}
\V{a}_1 \V{b}_1 & \V{a}_1 \V{b}_2 & \cdots & \V{a}_1 \V{b}_p \\
\V{a}_2 \V{b}_1 & \V{a}_2 \V{b}_2 & \cdots & \V{a}_2 \V{b}_p \\
\vdots          & \vdots          & \ddots & \vdots          \\
\V{a}_m \V{b}_1 & \V{a}_m \V{b}_2 & \cdots & \V{a}_m \V{b}_p
\end{bmatrix}
% \\
% &= \begin{bmatrix} A \V{b}_1 & A \V{b}_2 & \cdots & A \V{b}_p \end{bmatrix}
\qedhere
\end{align*}
\end{defn}

Hvis vi lar $A$ og~$B$ være som i definisjonen, kan vi også skrive
produktet slik:
\[
AB = \begin{bmatrix} A \V{b}_1 & A \V{b}_2 & \cdots & A \V{b}_p \end{bmatrix}
\]

\begin{ex}
La $A$ og~$B$ og~$C$ være følgende matriser:
\[
A =
\begin{bmatrix}
5 & 0 & -2 \\
3 & 1 &  4
\end{bmatrix}
\qquad
B =
\begin{bmatrix}
2 & 1 \\
1 & 0 \\
2 & 4
\end{bmatrix}
\qquad
C =
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
\]
Siden $A$ er en $2 \times 3$-matrise og $B$ er en
$3 \times 2$-matrise, er $AB$ en $2 \times 2$-matrise.  Vi regner ut
denne matrisen ved å bruke definisjonen:
\begin{align*}
AB
&=
\begin{bmatrix}
5 \cdot 2 + 0 \cdot 1 + (-2) \cdot 2 &
5 \cdot 1 + 0 \cdot 0 + (-2) \cdot 4 \\
3 \cdot 2 + 1 \cdot 1 +    4 \cdot 2 &
3 \cdot 1 + 1 \cdot 0 +    4 \cdot 4
\end{bmatrix}
\\
&=
\begin{bmatrix}
 6 & -3 \\
15 & 19
\end{bmatrix}
\end{align*}
Hvis vi ganger sammen de samme to matrisene i motsatt rekkefølge, får
vi en $3 \times 3$-matrise:
\begin{align*}
BA
&=
\begin{bmatrix}
2 \cdot 5 + 1 \cdot 3 &
2 \cdot 0 + 1 \cdot 1 &
2 \cdot (-2) + 1 \cdot 4 \\
1 \cdot 5 + 0 \cdot 3 &
1 \cdot 0 + 0 \cdot 1 &
1 \cdot (-2) + 0 \cdot 4 \\
2 \cdot 5 + 4 \cdot 3 &
2 \cdot 0 + 4 \cdot 1 &
2 \cdot (-2) + 4 \cdot 4
\end{bmatrix}
\\
&=
\begin{bmatrix}
13 &  1 &  0 \\
 5 &  0 & -2 \\
22 &  4 & 12
\end{bmatrix}
\end{align*}
Produktet av matrisene $B$ og~$C$ blir en $3 \times 2$-matrise:
\[
BC =
\begin{bmatrix}
 5 &  8 \\
 1 &  2 \\
14 & 20
\end{bmatrix}
\]
Men produktet $CB$ er ikke definert, siden $C$ er en
$2 \times 2$-matrise og $B$ en $3 \times 2$-matrise.

Vi kunne også regnet ut for eksempel $CA$, men $AC$ er ikke definert.
\end{ex}

Vi kan nå legge merke til at matrisemultiplikasjon -- i motsetning til
multiplikasjon av vanlige tall -- ikke er kommutativt.  Det vil si at
faktorenes rekkefølge spiller en rolle: $AB$ er ikke nødvendigvis det
samme som~$BA$.

Vi må altså passe på at vi ikke av gammel vane bytter om faktorene når
vi jobber med et produkt av matriser.  Men mange andre regneregler
fungerer like bra med matriser som med tall.  Vi tar et teorem med
noen regneregler for matrisemultiplikasjon.

\begin{thm}
La $A$, $B$ og~$C$ være matriser, $\V{v}$ en vektor, og $c$ et tall.
I hver del av teoremet antar vi at størrelsene på matrisene og
vektoren er slik at alle operasjonene som brukes er definert.
\begin{enumerate}
\item[(a)]
Matrisemultiplikasjon er en assosiativ operasjon:
\[
A(BC) = (AB)C
\]
Et spesialtilfelle av dette er følgende:
\[
(AB) \V{v} = A (B \V{v})
\]
\item[(b)]
Å skalere et matriseprodukt er det samme som å skalere én av faktorene
og deretter multiplisere:
\[
(c A) B = c (AB) = A (cB)
\]
\item[(c)]
Matrisemultiplikasjon distribuerer over addisjon av matriser:
\[
A(B+C) = AB + AC
\quad\text{og}\quad
(A+B)C = AC + BC
\]
\end{enumerate}
\end{thm}

% TODO:

% AB = AC =\> B = C

% AB = 0 =\> A=0 v B=0


\section*{Transponering}

Vi har hittil snakket om aritmetiske operasjoner på matriser som
tilsvarer operasjoner vi kan gjøre med tall: addisjon og
multiplikasjon.  Operasjonen \emph{transponering}, derimot, er helt
spesiell for matriser, og går ut på at vi bytter om rader og kolonner.

\begin{defn}
La
\[
A =
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}
\]
være en $m \times n$-matrise.  Den \defterm{transponerte} av~$A$ er
$n \times m$-matrisen
\[
A\tr =
\begin{bmatrix}
a_{11} & a_{21} & \cdots & a_{m1} \\
a_{12} & a_{22} & \cdots & a_{m2} \\
\vdots & \vdots & \ddots & \vdots \\
a_{1n} & a_{2n} & \cdots & a_{mn}
\end{bmatrix}
\]
der radene og kolonnene i~$A$ er byttet om.
\end{defn}

\begin{ex}
Hvis vi lar $A$ være matrisen
\[
A =
\begin{bmatrix}
5 & 0 & -2 \\
3 & 1 &  4
\end{bmatrix},
\]
så er den transponerte av~$A$ gitt ved:
\[
A\tr =
\begin{bmatrix}
 5 & 3 \\
 0 & 1 \\
-2 & 4
\end{bmatrix}
\]
Hvis vi transponerer denne matrisen igjen, så kommer vi tilbake til
utgangspunktet:
\[
(A\tr)\tr = A\qedhere
\]
\end{ex}

Vi tar med noen regneregler for transponering.

\begin{thm}
For enhver matrise~$A$ har vi at å transponere to ganger gir den
opprinnelige matrisen:
\[
(A\tr)\tr = A
\]
Hvis $A$ og~$B$ er matriser sik at produktet~$AB$ er definert, så er
den transponerte av produktet lik produktet av de transponerte, i
motsatt rekkefølge:
\[
(AB)\tr = B\tr \cdot A\tr
\]
\end{thm}


\section*{Identitetsmatriser}

Når vi ser på multiplikasjon av tall, så er det ett bestemt tall som
oppfører seg helt spesielt, nemlig tallet~$1$.  Tallet~$1$ oppfører
seg som et \emph{identitetselement} med hensyn på multiplikasjon.  Det
betyr at å gange med~$1$ ikke endrer noe.  Hvis vi starter med et
hvilket som helst tall, og ganger det med~$1$, så får vi bare det
samme tallet som resultat:
\[
a \cdot 1 = a
\]

Finnes det noe tilsvarende som dette i verdenen av matriser og
matrisemultiplikasjon?  Med andre ord: Finnes det en matrise~$I$ slik
at
\[
A \cdot I = A
\]
for alle matriser~$A$?

Det er ganske tydelig at det ikke gir mening å stille akkurat det
spørsmålet, for det går ikke an å finne én matrise~$I$ som er slik at
produktet~$AI$ er definert for alle matriser~$A$ av alle mulige
størrelser.

Så vi må begrense oss til å se på matriser av én størrelse om gangen.
Dessuten må vi huske på at matriseproduktet ikke er kommutativt, så
$AI$ og~$IA$ er ikke nødvendigvis det samme.  For å si at $I$ er et
identitetselement vil vi at både $AI$ og~$IA$ skal bli lik~$A$.

Det viser seg at å finne identitetselementer bare er mulig hvis vi
dessuten begrenser oss til kvadratiske matriser.  Da kan vi
reformulere spørsmålet vårt slik:

Finnes det en $n \times n$-matrise~$I_n$ som er slik at
\[
A \cdot I_n = A = I_n \cdot A
\]
for alle $n \times n$-matriser~$A$?

Det er ikke vanskelig å se at denne matrisen oppfyller egenskapene vi
er ute etter:
\[
I_n =
\underbrace{
\begin{bmatrix}
1      & 0      & 0      & \cdots & 0 \\
0      & 1      & 0      & \cdots & 0 \\
0      & 0      & 1      & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0      & 0      & 0      & \cdots & 1
\end{bmatrix}
}_{\text{$n \times n$-matrise}}
\]
Matrisen~$I_n$ er en $n \times n$-matrise der det er $1$-tall langs
diagonalen mellom øverste venstre hjørne og nederste høyre hjørne, og
bare~$0$ ellers.  Den kalles \emph{identitetsmatrisen} av
størrelse~$n$.

\begin{ex}
Identitetsmatrisen av størrelse~$2$ er:
\[
I_2 =
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
\]
Vi sjekker enkelt at vi kan gange en hvilken som helst
$2 \times 2$-matrise med~$I_2$, til venstre eller høyre, uten at noe
endres:
\begin{align*}
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
&=
\begin{bmatrix}
a_{11} \cdot 1 + a_{12} \cdot 0 &
a_{11} \cdot 0 + a_{12} \cdot 1 \\
a_{21} \cdot 0 + a_{22} \cdot 1 &
a_{21} \cdot 1 + a_{22} \cdot 0
\end{bmatrix}
\\
&=
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{bmatrix}
\\[4pt]
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{bmatrix}
&=
\begin{bmatrix}
1 \cdot a_{11} + 0 \cdot a_{21} &
1 \cdot a_{12} + 0 \cdot a_{22} \\
0 \cdot a_{11} + 1 \cdot a_{21} &
0 \cdot a_{12} + 1 \cdot a_{22}
\end{bmatrix}
\\
&=
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{bmatrix}
\end{align*}
Vi ser altså at identitetsmatrisen~$I_2$ oppfører seg som et
identitetselement med hensyn på multiplikasjon av
$2 \times 2$-matriser.
\end{ex}

Vi ser også at det å gange en identitetsmatrise med en vektor ikke
endrer vektoren.  Altså: Hvis $\V{v}$ er en vektor i~$\R^n$, så er
\[
I_n \cdot \V{v} = \V{v}.
\]
Mer generelt har vi at om vi ganger en identitetsmatrise med en
hvilken som helst matrise som den kan ganges med, så får vi den
matrisen som resultat.  Altså: Hvis $A$ er en $m \times n$-matrise, så
har vi:
\[
I_m \cdot A = A = A \cdot I_n
\]


\section*{Potenser av matriser}

Hvis $A$ er en kvadratisk matrise, så kan vi gange $A$ med seg selv.
Vi definerer potenser av~$A$ på tilsvarende måte som potenser av tall:
\begin{align*}
A^2 &= A \cdot A, \\
A^3 &= A \cdot A \cdot A,
\end{align*}
og så videre.  Generelt definerer vi at $A$ opphøyd i $n$-te er
produktet av $A$ med seg selv $n$~ganger:
\[
A^n = \underbrace{A \cdot A \cdot \cdots \cdot A}_{\text{$n$~ganger}}
\]
Et spesielt tilfelle er å opphøye i~$0$-te.  For tall har vi definert
at $a^0 = 1$.  Men vi vet jo at for matriser spiller
identitetsmatrisen den samme rollen som $1$ gjør for tall.  Derfor
definerer vi at en $n \times n$-matrise opphøyd i~$0$-te blir
identitetsmatrisen av størrelse~$n$:
\[
A^0 = I_n
\]


\section*{Inverser}

For multiplikasjon av tall har vi identitetselementet~$1$, og vi har
dessuten \emph{inverser}.  Gitt et tall~$a$ finnes et tall~$b$,
inversen til~$a$, som er slik at
\[
a \cdot b = 1.
\]
Inversen til~$a$ er selvfølgelig bare tallet~$1/a$.  For eksempel:
Inversen til tallet~$5$ er $1/5$, og inversen til $3/4$ er $4/3$.

Kan vi på tilsvarende måte finne inverser til matriser?  Igjen
begrenser vi oss til å se på kvadratiske matriser, og spørsmålet blir:
Gitt en $n \times n$-matrise~$A$, finnes det en matrise $B$ som er
slik at likhetene
\[
A \cdot B = I_n = B \cdot A
\]
er oppfylt?

Vi tar et eksempel for å se hvordan noen slike matriser kan se ut.

\begin{ex}
\label{ex:invers}
La $A$ og~$B$ være følgende $2 \times 2$-matriser:
\[
A =
\begin{bmatrix}
 4 & -2 \\
 3 & -1
\end{bmatrix}
\quad\text{og}\quad
B =
\begin{bmatrix}
-1/2 & 1 \\
-3/2 & 2
\end{bmatrix}
\]
Da kan vi regne ut at
\[
A \cdot B
= 
\begin{bmatrix}
 4 & -2 \\
 3 & -1
\end{bmatrix}
\begin{bmatrix}
-1/2 & 1 \\
-3/2 & 2
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
= I_2
\]
og
\[
B \cdot A
= 
\begin{bmatrix}
-1/2 & 1 \\
-3/2 & 2
\end{bmatrix}
\begin{bmatrix}
 4 & -2 \\
 3 & -1
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
= I_2.
\]
Disse matrisene oppfyller altså likhetene
\[
A \cdot B = I_2 = B \cdot A.\qedhere
\]
\end{ex}

Vi definerer begrepet «invers» ved å bruke disse likhetene.

\begin{defn}
La $A$ være en $n \times n$-matrise.  En \defterm{invers} til~$A$ er
en $n \times n$-matrise~$B$ som er slik at
\[
A \cdot B = I_n = B \cdot A.
\]
En matrise er \defterm{inverterbar} hvis den har en invers.
\end{defn}

Denne definisjonen gir opphav til noen åpenbare spørsmål:
\begin{itemize}
\item Finnes det kvadratiske matriser som ikke har noen invers?  (Vi
har gitt et eget navn, «inverterbar», til matriser som har invers.
Dette hinter ganske sterkt om at det bør finnes matriser som ikke har
invers også.)
\item Kan en matrise ha mer enn én invers?
\end{itemize}

Vi kan ganske enkelt besvare det første spørsmålet ved å se på et
eksempel.

\begin{ex}
Er matrisen
\[
A =
\begin{bmatrix}
1 & 1 \\
0 & 0
\end{bmatrix}
\]
inverterbar?

La oss skje hva som skjer hvis vi antar at det finnes en matrise
\[
B =
\begin{bmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22}
\end{bmatrix}
\]
som er en invers til~$A$.  Da må vi ha at $AB = I_2$, altså:
\[
\begin{bmatrix}
1 & 1 \\
0 & 0
\end{bmatrix}
\begin{bmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22}
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
\]
Men produktet på venstre side her er:
\[
\begin{bmatrix}
b_{11} + b_{21} & b_{12} + b_{22} \\
0               & 0
\end{bmatrix}
\]
Dermed får vi (ved å se på tallene nederst til høyre i matrisene som
skal være like) at $0 = 1$, noe som ikke er mulig.  Det kan altså ikke
finnes noen slik $B$ som er en invers til~$A$, så $A$ er ikke
inverterbar.
\end{ex}

La oss nå ta for oss spørsmålet om hvorvidt samme matrise kan ha flere
inverser.  Svaret er at det kan den ikke, og det kan vi vise ganske
enkelt ut fra definisjonen av invers.

\begin{thm}
Hvis en matrise er inverterbar, så har den nøyaktig én invers.
\end{thm}
\begin{proof}
La $A$ være en inverterbar $n \times n$-matrise.  Anta at $B$ er en
invers til~$A$, og at $C$ også er en invers til~$A$; det vil si at
\[
A \cdot B = I_n = B \cdot A
\qquad\text{og}\qquad
A \cdot C = I_n = C \cdot A.
\]
Vi vil vise at $B$ og~$C$ ikke kan være forskjellige, altså at vi nå
må ha $B = C$.

La oss ta utgangspunkt i produktet $BAC$.  Dette kan vi skrive som
enten $(BA) \cdot C$ eller $B \cdot (AC)$, og i hvert tilfelle får vi
(ved å bruke likhetene over) at uttrykket i parentes blir
identitetsmatrisen.  Vi setter dette sammen og får:
\[
C
 = I_n \cdot C
 = (BA) \cdot C
 = B \cdot (AC)
 = B \cdot I_n
 = B
\]
Vi har altså kommet frem til at $B = C$, så inversen er entydig.
\end{proof}

Nå som vi vet at en matrise~$A$ ikke kan ha mer enn én invers, kan vi
slutte å snakke om «en invers til~$A$» i ubestemt form.  Isteden sier
vi «inversen til~$A$» i bestemt form.  Vi definerer dessuten en
notasjon for inversen.  Hvis $A$ er en inverterbar matrise, så skriver
vi $A^{-1}$ for inversen til~$A$.

\begin{ex}
I eksempel~\ref{ex:invers} er matrisen~$B$ en invers til~$A$; vi har
altså at $A^{-1} = B$.  Vi får dessuten at $A$ er en invers til~$B$,
slik at $B^{-1} = A$.
\end{ex}

Hvorfor er inverser interessante?  Én grunn er at de kan fortelle oss
noe om løsninger av likninger.

Hvis vi skal løse en likning
\[
ax = b
\]
der $a$ og~$b$ er tall, så vil vi selvfølgelig dele på~$a$ for å få
$x$ alene på venstresiden.  Det er det samme som å gange med inversen
til~$a$.

Når vi skal løse en matriselikning
\[
A \V{x} = \V{b},
\]
så kan vi ikke dele på~$A$.  Men hvis $A$ er inverterbar, så kan vi
nesten gjøre det likevel, for da kan vi gange med inversen til~$A$.
Det gjør at vi kan konkludere med at likningen er løsbar -- uansett
hva høyresidevektoren~$\V{b}$ er -- og dessuten at løsningen må være
entydig.  Vi skriver opp dette som et teorem.

\begin{thm}
\label{thm:invers-Ax=b}
La $A$ være en $n \times n$-matrise, og $\V{b}$ en vektor i~$\R^n$.
Hvis $A$ er inverterbar, så har likningen $A \V{x} = \V{b}$ entydig
løsning, og løsningen er $\V{x} = A^{-1} \cdot \V{b}$.
\end{thm}
\begin{proof}
Vi sjekker ved innsetting at $\V{x} = A^{-1} \cdot \V{b}$ er en
løsning av likningen.  Vi har:
\[
A \cdot (A^{-1} \cdot \V{b})
 = (A \cdot A^{-1}) \cdot \V{b}
 = I_n \cdot \V{b}
 = \V{b}
\]
Det betyr at $\V{x} = A^{-1} \cdot \V{b}$ er en løsning.

Nå må vi sjekke at den er entydig.  Fra likningen $A \V{x} = \V{b}$
får vi ved å gange til venstre med $A^{-1}$ på begge sider av
likhetstegnet:
\[
A^{-1} A \V{x} = A^{-1} \V{b}
\]
Men siden $A^{-1} A = I_n$ kan venstresiden her forenkles til
$I_n \V{x}$, som bare er~$\V{x}$.  Dermed har vi:
\[
\V{x} = A^{-1} \V{b}
\]
Det betyr at dette er den eneste løsningen av likningen, og beviset er
ferdig.
\end{proof}

\begin{ex}
La oss se på følgende likning:
\[
\begin{bmatrix}
 4 & -2 \\
 3 & -1
\end{bmatrix}
\V{x}
=
\vv{2}{5}
\]
Fra eksempel~\ref{ex:invers} vet vi at matrisen på venstresiden av
denne likningen er inverterbar, og at inversen er:
\[
\begin{bmatrix}
-1/2 & 1 \\
-3/2 & 2
\end{bmatrix}
\]
Da sier teorem~\ref{thm:invers-Ax=b} at likningen har entydig løsning,
og at løsningen er:
\[
\V{x} =
\begin{bmatrix}
-1/2 & 1 \\
-3/2 & 2
\end{bmatrix}
\vv{2}{5}
=
\vv{4}{7}
\qedhere
\]
\end{ex}

Nå vet vi ganske mye om inverser av matriser -- men ikke hvordan vi
regner ut en invers.  Vi kommer til dette snart, men først skal vi se
på en generell teknikk for å løse flere likningssystemer samtidig.
Denne teknikken skal vi deretter bruke til å utlede en metode for å
regne ut inverser.


% \section*{Eksistens av inverser}

% \begin{thm}
% La $A$ være en $m \times n$-matrise.
% Følgende påstander er ekvivalente:
% \begin{enumerate}
% \item $A$ har en venstreinvers.
% \item Likningen $A \V{x} = \V{b}$ har løsning for alle vektorer~$\V{b}$ i~$\R^n$.
% \item Kolonnene i~$A$ utspenner~$\R^n$.
% \item Når vi gausseliminerer~$A$, får vi en matrise med pivotelementer i alle rader.
% \end{enumerate}
% \end{thm}



% \section*{Matriser og likningssystemer} % TODO ???



\section*{Samtidig løsning av flere systemer}

Vi husker at et lineært likningssystem kan skrives som en
matriselikning $A \V{x} = \V{b}$, og at vi løser det ved å
gausseliminere totalmatrisen
$\begin{amatrix}{1} A & \V{b} \end{amatrix}$.

Anta nå at vi vil løse flere systemer
\begin{align*}
A \V{x}_1 &= \V{b}_1 \\
A \V{x}_2 &= \V{b}_2 \\
          &\ \ \vdots \\
A \V{x}_t &= \V{b}_t
\end{align*}
med samme koeffisientmatrise~$A$, men forskjellige høyresidevektorer
$\V{b}_1$, $\V{b}_2$, \ldots, $\V{b}_t$.  Det kan vi selvsagt gjøre
ved å utføre gausseliminasjon på totalmatrisene til alle systemene:
\begin{align*}
& \begin{amatrix}{1} A & \V{b}_1 \end{amatrix} \\
& \begin{amatrix}{1} A & \V{b}_2 \end{amatrix} \\
& \qquad\vdots \\
& \begin{amatrix}{1} A & \V{b}_t \end{amatrix}
\end{align*}
Men da gjør vi egentlig den samme gausseliminasjonen mange ganger.
Det eneste som blir forskjellig er hva vi får i siste kolonne.  Vi kan
spare oss for arbeid ved å slå sammen totalmatrisene til den ene
matrisen
\[
\left[
\begin{array}{c|cccc} A & \V{b}_1 & \V{b}_2 & \cdots & \V{b}_t \end{array}
\right],
\]
og gausseliminere den.

\begin{ex}
La $A$ være følgende matrise:
\[
A =
\begin{bmatrix}
2 & -2 \\
1 &  3
\end{bmatrix}
\]
Vi vil løse disse tre systemene:
\[
A \V{x}_1 = \vv{-2}{7}
\qquad
A \V{x}_2 = \vv{10}{1}
\qquad
A \V{x}_3 = \vv{-4}{0}
\]
Vi lager en kombinert totalmatrise for alle systemene, og
gausseliminerer den:
\begin{align*}
\left[
\begin{array}{cc|ccc}
2 & -2 & -2 & 10 & -4 \\
1 &  3 &  7 &  1 &  0
\end{array}
\right]
&\roweq
\left[
\begin{array}{cc|ccc}
1 &  3 &  7 &  1 &  0 \\
2 & -2 & -2 & 10 & -4
\end{array}
\right]
\\
&\roweq
\left[
\begin{array}{cc|ccc}
1 &  3 &   7 &  1 &  0 \\
0 & -8 & -16 &  8 & -4
\end{array}
\right]
\\
&\roweq
\left[
\begin{array}{cc|ccc}
1 & 3 & 7 &  1 &   0 \\
0 & 1 & 2 & -1 & 1/2
\end{array}
\right]
\\
&\roweq
\left[
\begin{array}{cc|ccc}
1 & 0 & 1 &  4 & -3/2 \\
0 & 1 & 2 & -1 &  1/2
\end{array}
\right]
\end{align*}
Den siste matrisen her er på redusert trappeform, og nå kan vi finne
løsningene av de tre systemene ved å se på de tre høyresidene i denne
matrisen:
\[
\V{x}_1 = \vv{1}{2}
\qquad
\V{x}_2 = \vv{4}{-1}
\qquad
\V{x}_3 = \vv{-3/2}{1/2}
\qedhere
\]
\end{ex}


\section*{Beregning av inverser}

La oss nå se på hvordan vi kan regne ut inverser.  Anta at vi har en
$n \times n$-matrise~$A$.  Vi vil finne ut om den er inverterbar, og i
så fall vil vi finne inversmatrisen~$A^{-1}$.

Vi ser på likningen
\[
AX = I_n,
\]
der $X$ er en ukjent $n \times n$-matrise.  Hvis vi lar
\[
\V{x}_1,\ \V{x}_2,\ \ldots,\ \V{x}_n
\]
være kolonnene i~$X$, altså
\[
X = \begin{bmatrix} \V{x}_1 & \V{x}_2 & \cdots & \V{x}_n \end{bmatrix},
\]
så kan vi skrive produktet $AX$ slik:
\[
AX = \begin{bmatrix} A \V{x}_1 & A \V{x}_2 & \cdots & A \V{x}_n \end{bmatrix}
\]
La oss gi navn til kolonnene i identitetsmatrisen~$I_n$ også:
\[
I_n = \begin{bmatrix} \V{e}_1 & \V{e}_2 & \ldots & \V{e}_n \end{bmatrix}
\]
Det vil si at $\V{e}_i$ er den vektoren i~$\R^n$ som har et $1$-tall
på posisjon~$i$, og bare $0$-er ellers.

Nå kan vi, ved å se på hver kolonne, skrive om likningen $AX = I_n$
til disse $n$ likningene:
\begin{align*}
A \V{x}_1 &= \V{e}_1 \\
A \V{x}_2 &= \V{e}_2 \\
          &\ \ \vdots \\
A \V{x}_n &= \V{e}_n
\end{align*}
Dermed kan vi bruke teknikken vi beskrev over for å løse flere
likningssystemer samtidig.  Da må vi gauss\-eliminere matrisen
\[
\left[
\begin{array}{c|cccc} A & \V{e}_1 & \V{e}_2 & \cdots & \V{e}_n \end{array}
\right]
\]
for å løse disse systemene.

La oss ta et eksempel for å se hvordan dette blir i praksis.

\begin{ex}
\label{ex:invertering}
Vi vil forsøke å invertere følgende matrise:
\[
A =
\begin{bmatrix}
1 & -1 & -3 \\
0 &  1 &  3 \\
2 & -2 & -1
\end{bmatrix}
\]
Vi følger ideene beskrevet over, så vi finner en matrise $X$ slik at
$AX = I_3$ ved å løse følgende tre systemer:
\[
A \V{x}_1 = \vvv{1}{0}{0}
\quad
A \V{x}_2 = \vvv{0}{1}{0}
\quad
A \V{x}_3 = \vvv{0}{0}{1}
\]
Vi setter opp den kombinerte totalmatrisen for de tre systemene og
gausseliminerer:
\begin{align*}
\left[
\begin{array}{@{}ccc|ccc@{}}
1 & -1 & -3 & 1 & 0 & 0 \\
0 & 1 & 3 & 0 & 1 & 0 \\
2 & -2 & -1 & 0 & 0 & 1
\end{array}
\right]
&
\roweq
\left[
\begin{array}{ccc|ccc}
1 & -1 & -3 & 1 & 0 & 0 \\
0 & 1 & 3 & 0 & 1 & 0 \\
0 & 0 & 5 & -2 & 0 & 1
\end{array}
\right]
\\
&\roweq
\left[
\begin{array}{ccc|ccc}
1 & -1 & -3 & 1 & 0 & 0 \\
0 & 1 & 3 & 0 & 1 & 0 \\
0 & 0 & 1 & -\frac{2}{5} & 0 & \frac{1}{5}
\end{array}
\right]
\\
&\roweq
\left[
\begin{array}{ccc|ccc}
1 & 0 & 0 & 1 & 1 & 0 \\
0 & 1 & 3 & 0 & 1 & 0 \\
0 & 0 & 1 & -\frac{2}{5} & 0 & \frac{1}{5}
\end{array}
\right]
\\
&\roweq
\left[
\begin{array}{ccc|ccc}
1 & 0 & 0 & 1 & 1 & 0 \\
0 & 1 & 0 & \frac{6}{5} & 1 & -\frac{3}{5} \\
0 & 0 & 1 & -\frac{2}{5} & 0 & \frac{1}{5}
\end{array}
\right]
\end{align*}
Vi får altså følgende løsninger:
\[
\V{x}_1 = \vvv{1}{6/5}{-2/5}
\quad
\V{x}_2 = \vvv{1}{1}{0}
\quad
\V{x}_3 = \vvv{0}{-3/5}{1/5}
\]
Nå finner vi matrisen~$X$ ved å bruke $\V{x}_1$, $\V{x}_2$
og~$\V{x}_3$ som kolonner:
\[
X =
\begin{bmatrix}
1 & 1 & 0 \\
6/5 & 1 & -3/5 \\
-2/5 & 0 & 1/5
\end{bmatrix}
\]
Vi har funnet $X$ ved å løse likningen $AX = I_3$, så vi vet at dette
stemmer.  Det kan likevel være lurt å sjekke det ved å gange sammen
$A$ og~$X$, for å være sikre på at vi ikke har regnet feil.

Hvis du prøver å gange dem sammen motsatt vei, vil du oppdage at vi
også har $XA = I_3$.  Dette betyr at $X$ er inversen til~$A$, altså at
$A^{-1} = X$.
\end{ex}

I eksempelet løste vi likningen $AX = I_3$, og det viste seg at
matrisen~$X$ som vi fant også oppfylte likheten $XA = I_3$, slik at vi
kunne konkludere med at $A^{-1} = X$.

Dette var ikke en tilfeldighet -- det er faktisk alltid nok å løse
likningen $AX = I_n$ for å finne inversen til~$A$.  Vi skal bevise
dette, men vi tar først et lemma (hjelperesultat) som vi skal bruke i
beviset vårt.

\begin{lem}
\label{lem:invers}
La $A$ og~$B$ være $n \times n$-matriser.  Dersom
\[
\begin{amatrix}{1} A & I_n \end{amatrix}
\roweq
\begin{amatrix}{1} I_n & B \end{amatrix},
\]
så er $AB = I_n$.
\end{lem}
\begin{proof}
Vi viste over (i diskusjonen før eksempel~\ref{ex:invertering}) at vi kan
løse likningen $AX = I_n$ ved å gausseliminere matrisen
\[
\begin{amatrix}{1} A & I_n \end{amatrix}.
\]
Nå har vi antatt at
\[
\begin{amatrix}{1} A & I_n \end{amatrix}
\roweq
\begin{amatrix}{1} I_n & B \end{amatrix},
\]
og siden den andre matrisen her er på redusert trappeform, er det den
vi ender opp med når vi gausseliminerer.  Det vil si at $X = B$ er
løsningen av likningen $AX = I_n$, altså har vi $AB = I_n$.
\end{proof}

Nå er vi klare for å bevise at metoden vår for å finne inverser
fungerer.

\begin{thm}
\label{thm:invers}
La $A$ være en $n \times n$-matrise.
\begin{enumerate}
\item[(a)] $A$ er inverterbar hvis og bare hvis $A \roweq I_n$.
\item[(b)] Hvis $A$ er inverterbar, så kan vi finne inversen ved å
gausseliminere matrisen
\[
\begin{amatrix}{1} A & I_n \end{amatrix}
\]
til redusert trappeform og lese av høyre halvdel av den resulterende
matrisen.  Med andre ord: Resultatet av gausseliminasjonen blir
følgende matrise:
\[
\begin{amatrix}{1} I_n & A^{-1} \end{amatrix}
\]
\end{enumerate}
\end{thm}
\begin{proof}
Når vi gausseliminerer matrisen
\[
\begin{amatrix}{1} A & I_n \end{amatrix}
\]
til redusert trappeform, må venstre halvdel av den resulterende
matrisen enten bli $I_n$, eller en matrise med minst én nullrad.  Men
i høyre halvdel kan det ikke bli noen nullrader, for enhver rad vi får
etter å ha gjort radoperasjoner på $I_n$ er på formen
\[
a_1 \V{r}_1 + a_2 \V{r}_2 + \cdots + a_n \V{r}_n
\]
der $\V{r}_1$, $\V{r}_2$, \ldots, $\V{r}_n$ er radene i $I_n$ og minst
én $a_i$ er ulik~$0$.  Dette betyr at hvis vi får en nullrad i venstre
halvdel av trappeformmatrisen, så har likningen $AX = I_n$ ingen
løsning, og dermed er $A$ ikke inverterbar.  Dermed har vi vist én
halvdel av påstanden i del~(a): Hvis $A$ er inverterbar, så må vi ha
$A \roweq I_n$.

La oss nå anta at $A \roweq I_n$.  Vi vil vise at da er $A$
inverterbar, og at metoden beskrevet i del~(b) gir riktig svar.  La
$B$ være matrisen vi får som svar ved å bruke denne metoden.  Det vil
si at følgende matriser er begynnelsen og slutten av
gausseliminasjonen vi utfører:
\[
\begin{amatrix}{1} A & I_n \end{amatrix}
\roweq
\begin{amatrix}{1} I_n & B \end{amatrix},
\]
Nå sier lemma~\ref{lem:invers} at $AB = I_n$.

La oss stokke litt om på kolonnene i matrisen
\[
\begin{amatrix}{1} A & I_n \end{amatrix}
\]
og isteden se på følgende matrise:
\[
\begin{amatrix}{1} I_n & A \end{amatrix}
\]
Hvis vi utfører akkurat de samme radoperasjonene på denne som vi
gjorde i gausseliminasjonen av den første matrisen, så får vi akkurat
samme resultat, men med tilsvarende omstokking av kolonnene, altså:
\[
\begin{amatrix}{1} B & I_n \end{amatrix}
\]
Dette betyr at disse matrisene er radekvivalente:
\[
\begin{amatrix}{1} B & I_n \end{amatrix}
\roweq
\begin{amatrix}{1} I_n & A \end{amatrix}
\]
Ved å bruke lemma~\ref{lem:invers} igjen, på denne siste
radekvivalensen, får vi at $BA = I_n$.

Vi har altså vist at vi har
\[
AB = I_n = BA,
\]
som betyr at $B$ er inversen til~$A$.  Det vil si at vi har bevist
andre halvdel av del~(a) (hvis $A \roweq I_n$, så er $A$ inverterbar),
og vi har bevist at metoden i del~(b) gir riktig svar.
\end{proof}

Det alt dette betyr i praksis er at hvis vi har en matrise~$A$ som vi
har lyst til å invertere, hvis det er mulig, så setter vi opp matrisen
\[
\begin{amatrix}{1} A & I_n \end{amatrix}
\]
og gausseliminerer.  Da er det to muligheter.  Enten får vi en nullrad
i venstre halvdel, og da er $A$ ikke inverterbar.  Eller så kommer vi
frem til redusert trappeform uten noen nullrad i venstre halvdel, og
da har vi matrisen
\[
\begin{amatrix}{1} I_n & A^{-1} \end{amatrix}
\]
der inversen til~$A$ kan leses av i høyre halvdel.

% TODO thm side 130


\kapittelslutt
