\input{kapittel}

\kapittel{11}{Kompleks lineær algebra}
\label{ch:kompleks-linear-algebra}

I dette kapitlet er det fire sentrale poenger som skal drives gjennom:

\begin{itemize}
\item Skalarene i definisjonen av vektorrom kan være andre tall enn de reelle.

\item Lineæralgebra over $\mathbb C$ er stort sett helt likt som over $\R$.

\item Alle matriser har $n$ komplekse egenverdier om du teller med multiplisiteten deres.
 
\item Egenrommet har dimensjon mindre enn eller lik multiplisiteten til den tilhørende egenverdien.
\end{itemize}

\section*{Definisjonen av vektorrom II}

De rasjonale tallene $\Q$, de reelle tallene $\R$, 
og de komplekse tallene $\C$ er alle eksempler på et konsept kalt kropp. 
En kropp er et tallsystem med to regneoperasjoner $+$ og $\cdot$, 
som tilfredsstiller ti-tolv aksiomer, 
hvorav noen ligner ganske bra på vektorromsaksiomene. 
Vi skal ikke gå gjennom disse aksiomene, 
men sentralt for en kropp er at alle elementer i kroppen har multiplikativ invers.

Vektorromsaksiomene opererer med to ting som kombineres, nemlig skalarer og vektorer. 
Tidligere har vi bare sagt at vektorer kan ganges med skalarer, 
men ikke åpnet for at disse kan være noe annet enn reelle tall. 
Det skal vi gjøre noe med nå. 
Vi skal ikke endre på aksiomene for vektorrom, 
men vi skal begynne å spesifisere hva slags skalarer som kan ganges med vektorer. 

\begin{defn}
La $V$ være en mengde, og $K$ en kropp. De to operasjonene er fremdeles
\begin{align*}
\text{addisjon av vektorer: } & \V{u} + \V{v} \\
\text{skalarmultiplikasjon: } & c \cdot \V{u}
\end{align*}
Addisjonen er definert for alle elementer $\V{u}$ og~$\V{v}$
i~$V$, og skalarmultiplikasjonen for alle skalarer~$c$ i $K$ og alle $\V{u}$
i~$V$.  Resultatet av operasjonene skal alltid være et element i~$V$.
Dersom kombinasjonen~$V$ og $K$ oppfyller vektorromsaksiomene, 
så sier vi at $V$ er et \defterm{vektorrom over $K$}.
\end{defn}


\begin{defn}
Vi skriver $\C^n$ for vektorrommet der $K=\C$ og vektorene er kolonnevektorer med komplekse komponenter 
\[
\V z=
\begin{bmatrix}
a_1+b_1 i    \\ a_2+b_2 i   \\ \vdots \\ a_n+b_n i 
\end{bmatrix}
=
\begin{bmatrix}
z_1    \\ z_2   \\ \vdots \\ z_n
\end{bmatrix}.\qedhere
\]
\end{defn}

\begin{ex}
Standardbasisen 
\[
\begin{bmatrix}
1    \\ 0  \\ \vdots \\ 0
\end{bmatrix},
\begin{bmatrix}
0    \\ 1  \\ \vdots \\ 0
\end{bmatrix}
...
\begin{bmatrix}
0    \\ 0  \\ \vdots \\ 1
\end{bmatrix}
\]
for $\R^n$ er også en basis for $\C^n$. 
Det er lett å se at alle elementer i $\C^n$ kan skrives som en unik lineærkombinasjon av elementene i basisen. 
En tilfeldig vektor $\V z$ i $\C^n$ kan skrives
\begin{align*}
\V z&=
\begin{bmatrix}
a_1+b_1 i    \\ a_2+b_2 i   \\ \vdots \\ a_n+b_n i 
\end{bmatrix}
=
\begin{bmatrix}
z_1    \\ z_2   \\ \vdots \\ z_n
\end{bmatrix}
\\&=
z_1
\begin{bmatrix}
1    \\ 0  \\ \vdots \\ 0
\end{bmatrix}
+
z_2
\begin{bmatrix}
0    \\ 1  \\ \vdots \\ 0
\end{bmatrix}
+...+
z_n
\begin{bmatrix}
0    \\ 0  \\ \vdots \\ 1
\end{bmatrix}. \qedhere
\end{align*}
\end{ex}

\begin{ex}
Man kan også se på $\C^n$ som et vektorrom over $\R$, men da er ikke standardbasisen for $\R^n$ en basis for $\C^n$. En basis er
\begin{align*}
&
\begin{bmatrix}
1    \\ 0  \\ \vdots \\ 0
\end{bmatrix}
+
\begin{bmatrix}
0    \\ 1  \\ \vdots \\ 0
\end{bmatrix}
+...+
\begin{bmatrix}
0    \\ 0  \\ \vdots \\ 1
\end{bmatrix}
\\ +
&
\begin{bmatrix}
i    \\ 0  \\ \vdots \\ 0
\end{bmatrix}
+
\begin{bmatrix}
0    \\ i  \\ \vdots \\ 0
\end{bmatrix}
+...+
\begin{bmatrix}
0    \\ 0  \\ \vdots \\ i
\end{bmatrix},
\end{align*}
og vi ser at dimensjonen er $2n$. 
En tilfeldig vektor $\V z$ i $\C^n$ kan skrives
\begin{align*}
\V z&=
\begin{bmatrix}
a_1+b_1 i    \\ a_2+b_2 i   \\ \vdots \\ a_n+b_n i 
\end{bmatrix}
\\&= 
a_1
\begin{bmatrix}
1    \\ 0  \\ \vdots \\ 0
\end{bmatrix}
+
a_2
\begin{bmatrix}
0    \\ 1  \\ \vdots \\ 0
\end{bmatrix}
+...+
a_n
\begin{bmatrix}
0    \\ 0  \\ \vdots \\ 1
\end{bmatrix}
\\ &+
b_1
\begin{bmatrix}
i    \\ 0  \\ \vdots \\ 0
\end{bmatrix}
+
b_2
\begin{bmatrix}
0    \\ i  \\ \vdots \\ 0
\end{bmatrix}
+...+
b_n
\begin{bmatrix}
0    \\ 0  \\ \vdots \\ i
\end{bmatrix}. \qedhere
\end{align*}
\end{ex}

\begin{ex}
$\R^n$ et vektorrom over $\R$.
\end{ex}

\begin{ex}
$\Q^n$ et vektorrom over $\Q$. Selv om vi har sagt at vi har gjort lineær algebra på $\R^n$ til nå, har vi i praksis operert på $\Q^n$. Ikke et eneste matriseeksempel har involvert $\sqrt{2}$.
\end{ex}

\begin{ex}
$\P^n$ er et vektorrom over $\mathbb R$ slik vi har jobbet med det. 
Men det er ingenting i veien for å behandle polynomer der kroppen, variabelen og koeffisientene alle er komplekse.
\end{ex}


\begin{ex}
De hele tallene $\Z$ er ikke en kropp, så en del av teoremene vi har bevist, gjelder ikke for $\Z^n$. 
Vi har jo sett at et $n\times n$-likningssystem med komponenter i $\Z$ og lineært uavhengige kolonner,  
ikke nødvendigvis noen løsning i $\Z^n$. 
\end{ex}



\section*{Lineær algebra over $\C$}

Vi starter dette avsnittet med et litt suspekt teorem. Så tar vi noen eksempler på lineæralgebra over $\C$.

\begin{fishythm}
Alt vi har gjort til nå, fungerer helt likt i $\mathbb C^n$ som i $\R^n$. 
Noen ting fungerer til og med bedre.
\end{fishythm}




\begin{ex}
Vi begynner med å løse likningssystemet
\begin{align*}
(1-i) z + 3w   &= 2-3i \\
i z + (1+2i) w &= 1
\end{align*}
som har totalmatrise
\[
\begin{amatrix}{2}
1-i & 3 & 2-3i \\ i &1+2i & 1
\end{amatrix}.
\]
Vi ønsker å kvitte oss med $i$-en til venstre i den andre raden. 
Den første raden ganget  med $\frac{i}{1-i}$ er
\[
\begin{amatrix}{2}
i & \frac{3i}{1-i} & \frac{3+2i}{1-i} 
\end{amatrix}.
\]
Vi trekker dette fra den andre raden og erstatter den andre raden med resultatet:
\[
\begin{amatrix}{2}
1-i & 3 & 2-3i \\ 0 &1+2i -\frac{3i}{1-i} & 1-\frac{3+2i}{1-i} 
\end{amatrix}.
\]
Jeg tror vi ganger den andre raden med $1-i$ for å rydde litt:
\[
\begin{amatrix}{2}
1-i & 3 & 2-3i \\ 0 &3 -2i  & -2-3i
\end{amatrix}
\]
Vi er nå klare for å beregne $w$ og $z$:
\begin{align*}
w&=\frac{-2-3i}{3-2i}=\frac{-2-3i}{3-2i}\cdot \frac{3+2i}{3+2i}=-i \\
z&=\frac{2-3i-3(-i)}{1-i}=1+i \qedhere
\end{align*}
\end{ex}

\begin{ex}
Det kan verifiseres at inversen til 
\[
\begin{bmatrix}
1-i & 3  \\ i &1+2i 
\end{bmatrix}
\]
er
\[
\frac{1}{3-2i}
\begin{bmatrix}
1+2i & -3  \\ -i &1-i 
\end{bmatrix}
\]
ved å gange dem sammen:
\[
\frac{1}{3-2i}
\setlength\arraycolsep{4pt}
\begin{bmatrix}
1-i & 3  \\ i &1+2i 
\end{bmatrix}
\begin{bmatrix}
1+2i & -3  \\ -i &1-i 
\end{bmatrix}
=
\begin{bmatrix}
1 & 0  \\ 0 &1 
\end{bmatrix} \qedhere
\]


\end{ex}

\begin{ex}
Vi sjekker lineær uavhengighet akkurat som i det reelle tilfellet, altså ved å beregne nullrommet eller determinanten. For eksempel er kolonnene i matrisen 
\[
\begin{bmatrix}
2i & 3 & 4 \\ 3i & 4 & 5 \\ 4i &  5 &6  
\end{bmatrix}
\]
lineært avhengige, siden 
\[
-i
\begin{bmatrix}
2i  \\ 3i \\ 4i 
\end{bmatrix}
+
-2
\begin{bmatrix}
 3 \\  4  \\   5 
\end{bmatrix}
+
\begin{bmatrix}
 4 \\ 5 \\  6  
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0 \\ 0
\end{bmatrix}. \qedhere
\]
\end{ex}




%\section*{Komplekse polynomer}

%Algebraens fundamentalteorem sier at likningen
%\[
%a_nz^n+a_{n-1}z^{n-1}+...+a_1z+a_0=0
%\]
%alltid har $n$ komplekse løsninger, i den forstand at vi alltid kan skrive
%\[
%a_nz^n+a_{n-1}z^{n-1}+...+a_1z+a_0=\prod_{n=1}^n (z-z_i),
%\]
%der $z_i$ er polynomets røtter. Noen av disse kan være repeterte. Hvis en rot er repetert $k$ ganger, sier vi at den har \defterm{multiplisitet} $k$. 


%\begin{ex}
%Polynomet 
%\[
%=(z-1)(z+i)^2
%\]
%har en rot med multiplisitet ($z=1$) med multiplisitet 3.
%\end{ex}

\section*{Komplekse egenverdier}
Fra forrige kapittel husker vi at et $n$-te ordens polynom alltid kan faktoriseres i $n$ lineære faktorer. Dersom vi teller repeterte faktorer, er det vanlig å si at en polynomlikning 
\[
a_n z^n+a_{n-1} z^{n-1}+...+a_1 z +a_0=0
\]
har $n$ løsninger.

\begin{thm}
Det karakteristiske polynomet til en $n\times n$-matrise har alltid orden $n$. 
En matrise har alltid $n$ egenverdier dersom du 
teller med multiplisiteten til hver egenverdi. 
\end{thm}

Vi skal ta noen eksempler der vi beregner egenverdier. Vi beregner egenvektorene i neste avsnitt.

\begin{ex}
Matrisen
\[
\begin{bmatrix}
0 & -1 \\ 1 &0
\end{bmatrix}
\]
har visst egenverdier allikevel, se eksempel \ref{ex:ingen-egenverdier}. Det karakteristiske polynomet er
\[
\lambda^2+1,
\]
så egenverdiene er $\pm i$. 
\end{ex}

\begin{ex}
Matrisen
\[
\begin{bmatrix}
3 & 0 & 0 \\ 0 & 1 & -1 \\ 0&  1 &1  
\end{bmatrix}
\]
har karakteristisk polynom
\[
(3-\lambda)((1-\lambda)^2+1)=(3-\lambda)(2-2\lambda+\lambda^2).
\]
Den ene egenverdien er åpenbart $\lambda=3$, mens andregradspolynomet $2-2\lambda+\lambda^2$ har røtter
\[
\lambda=\frac{2\pm\sqrt{4-8}}{2}=1\pm i.
\] 
Her er det altså en reell egenverdi $3$, og to komplekse egenverdier $1+i$ og $1-i$.
\end{ex}

\begin{thm}
Egenverdiene til en reell matrise kommer i komplekskonjugerte par.
\end{thm}


\begin{ex}
Matrisen
\[
\begin{bmatrix}
1 & 1 & 0\\  0 &1 & 0 \\ 0 & 0 & 2
\end{bmatrix}
\]
har karakteristisk likning
\[
(2-\lambda)(1-\lambda)^2=0,
\]
med en enkel egenverdi $\lambda=2$, og en dobbel egenverdi $\lambda=1$. 
\end{ex}


\section*{Mer om egenrommet}
En $n \times n$-matrise har alltid $n$ egenverdier, 
men ikke nødvendigvis $n$ lineært uavhengige egenvektorer.

\begin{ex}
Matrisen
\[
\begin{bmatrix}
0 & -1 \\ 1 &0
\end{bmatrix}
\]
egenverdier
\[
\lambda=\pm i.
\] 
En egenrommet til $-i$ er nullrommet til 
\[
\begin{bmatrix}
i & -1 \\ 1 &i
\end{bmatrix}.
\]
Vi vet at denne matrisen er singulær, 
og da må radene være skalarmultipler av hverandre (i dette tilfellet er den nederste $i$ ganger den øverste), 
så vi kan egentlig bare stryke den nederste, og se at
\[
ix_1-x_2=0,
\]
slik at en egenvektor til $-i$ blir
\[
\begin{bmatrix}
1  \\ i
\end{bmatrix}
\]
Likeledes blir en egenvektor til $i$ 
\[
\begin{bmatrix}
i  \\ 1
\end{bmatrix}.
\]
Vi dobbeltsjekker
\[
\begin{bmatrix}
0 & -1 \\ 1 &0
\end{bmatrix}
\begin{bmatrix}
i  \\ 1
\end{bmatrix}
=
\begin{bmatrix}
-1  \\ i
\end{bmatrix}
=
i
\begin{bmatrix}
i  \\ 1
\end{bmatrix}.\qedhere
\]
\end{ex}

\begin{ex}
Vi beregner egenrommet til matrisen
\[
\begin{bmatrix}
3 & 0 & 0 \\ 0 & 1 & -1 \\ 0&  1 &1  
\end{bmatrix}
\]
sin egenverdi $\lambda= 1-i$. Dette er nullrommet til
\[
\begin{bmatrix}
2+i & 0 & 0 \\ 0 & i & -1 \\ 0&  1 &i  
\end{bmatrix}.
\]
Den øverste raden forteller at $x_1=0$. De to nedereste ligner mistenkelig på forrige eksempel, så en egenvektor blir 
\[
\begin{bmatrix}
0 \\ 1  \\ i 
\end{bmatrix}.
\]
Vi dobbeltsjekker
\[
\begin{bmatrix}
3 & 0 & 0 \\ 0 & 1 & -1 \\ 0&  1 &1  
\end{bmatrix}
\begin{bmatrix}
0 \\ 1  \\ i 
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 1-i  \\ 1+i 
\end{bmatrix}
=
(1-i)
\begin{bmatrix}
0 \\ 1  \\ i 
\end{bmatrix}. \qedhere
\]

\end{ex}


\begin{ex}
Matrisen
\[
\begin{bmatrix}
1 & 1 & 0\\  0 &1 & 0 \\ 0 & 0 & 2
\end{bmatrix}
\]
har dobbel egenverdi $\lambda=1$. Det tilhørende egenrommet er nullrommet til 
\[
\begin{bmatrix}
0 & 1 & 0\\  0 &0 & 0 \\ 0 & 0 & 1
\end{bmatrix}
\]
%altså ikketrivielle løsninger av 
%
%\[
%\begin{bmatrix}
%0 & 1 & 0\\  0 &0 & 0 \\ 0 & 0 & 1
%\end{bmatrix}
%\begin{bmatrix}
%x_1  \\ x_2 \\ x_3
%\end{bmatrix}
%=
%\begin{bmatrix}
%0  \\ 0 \\0
%\end{bmatrix}
%\]
Dette gir at
$
x_2=x_3=0,
$
så en egenvektor til $\lambda=1$ er
%\[
%\Sp \left\{ \vvv{1}{0}{0} \right\}.
%\]
\[
 \vvv{1}{0}{0}.
\]
Her er egenrommet endimensjonalt, mens egenverdien hadde multiplisitet 2.
\end{ex}


\begin{thm}
Egenrommet har dimensjon mindre enn eller lik multiplisiteten til egenverdien. 
\end{thm}

\noindent Dersom et egenrom har lavere dimensjon enn multiplisiteten til egenverdien, sier vi at egenverdien er \defterm{defekt}. 
Dersom en $n\times n$-matrise har $n$ lineært uavhengige egenvektorer, sier vi at den er \defterm{diagonaliserbar}.
Grunnen til dette navnet skal vi komme tilbake til.





\kapittelslutt
